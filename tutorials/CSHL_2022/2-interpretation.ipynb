{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "APNfq3knhfZg"
   },
   "source": [
    "# How to train your DragoNN tutorial 2: \n",
    "## Interpreting features induced by CNN's \n",
    "\n",
    "\n",
    "This tutorial will take 1 hour if executed on a GPU.\n",
    "\n",
    "## Outline<a name='outline'>\n",
    "<ol>\n",
    "    <li><a href=#1>How to use this tutorial</a></li>\n",
    "    <li><a href=#2>Review of patterns in transcription factor binding sites</a></li>\n",
    "    <li><a href=#3>Learning to localize homotypic motif density</a></li>\n",
    "    <li><a href=#4>Simulate training data with simdna</a></li>  \n",
    "    <li><a href=#4.5>Running dragonn on your own data: starting with FASTA files</a></li>\n",
    "    <li><a href=#5>Defining CNN architecture</a></li>\n",
    "    <li><a href=#9>Multi-layer model</a></li>\n",
    "    <li><a href=#8>Model Interpretation</a></li>    \n",
    "    <li><a href=#10>Regularized multi-layer model</a></li>\n",
    "    <li><a href=#12>Further exploration</a></li>    \n",
    "    <li><a href=#13>Using DragoNN command-line interface</a></li>\n",
    "</ol>\n",
    "Github issues on the dragonn repository with feedback, questions, and discussion are always welcome.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NixF5bW3hfZg"
   },
   "source": [
    "## How to use this tutorial<a name='1'>\n",
    "<a href=#outline>Home</a>\n",
    "\n",
    "This tutorial utilizes a Google Colaboratory Notebook - an interactive computational enviroment that combines live code, visualizations, and explanatory text. The notebook is organized into a series of cells. \n",
    "\n",
    "The first thing we do is set our Runtime to use Python3 and GPU. \n",
    "\n",
    "![ChangeRuntime](https://github.com/kundajelab/dragonn/blob/cshl/tutorials/tutorial_images/ChangeRuntime.png?raw=true)\n",
    "\n",
    "![RuntimeType.png](https://github.com/kundajelab/dragonn/blob/cshl/tutorials/tutorial_images/RuntimeType.png?raw=1)\n",
    "\n",
    "Now that we set our Runtime, we can execute the cells in the notebook. You can execute the cells one at a time by clicking inside of them and pressing SHIFT+enter. Alternatively, you can run all the cells by clicking the \"Run All\" button, as demonstrated below. \n",
    "\n",
    "![RunAllColab](https://github.com/kundajelab/dragonn/blob/cshl/tutorials/tutorial_images/RunAllCollab.png?raw=1)\n",
    "\n",
    "\n",
    "You can run the next cell by cliking the play button:\n",
    "\n",
    "![RunCellArrow](https://github.com/kundajelab/dragonn/blob/cshl/tutorials/tutorial_images/RunCellArrow.png?raw=1)\n",
    "\n",
    "Half of the cells in this tutorial contain code, the other half contain visualizations and explanatory text. Code, visualizations, and text in cells can be modified - you are encouraged to modify the code as you advance through the tutorial. You can inspect the implementation of a function used in a cell by following these steps:\n",
    "\n",
    "![inspecting code](https://github.com/kundajelab/dragonn/blob/cshl/paper_supplement/primer_tutorial_images/inspecting_code.png?raw=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wORsai47hfZi"
   },
   "outputs": [],
   "source": [
    "#uncomment the lines below if you are running this tutorial from Google Colab \n",
    "# RESTART NOTEBOOK AFTER RUNNING THIS\n",
    "!pip install git+https://github.com/kundajelab/dragonn.git@icts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/kundajelab/shap.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oSaW2fD2hfZk"
   },
   "outputs": [],
   "source": [
    "# Making sure our results are reproducible\n",
    "from numpy.random import seed\n",
    "seed(1234)\n",
    "from tensorflow.random import set_seed\n",
    "set_seed(1234)\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5-bvAg5-hfZn"
   },
   "source": [
    "We start by loading dragonn's tutorial utilities and reviewing properties of regulatory sequence that transcription factors bind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1e2J6BZ-hfZo"
   },
   "outputs": [],
   "source": [
    "#load dragonn tutorial utilities \n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7e3bYhMBhfZr"
   },
   "source": [
    "## Key properties of regulatory DNA sequences <a name='2'>\n",
    "<a href=#outline>Home</a>\n",
    "\n",
    "![sequence properties 1](https://github.com/kundajelab/dragonn/blob/master/paper_supplement/primer_tutorial_images/sequence_properties_1.jpg?raw=1)\n",
    "![sequence properties 2](https://github.com/kundajelab/dragonn/blob/master/paper_supplement/primer_tutorial_images/sequence_properties_2.jpg?raw=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ga1AXeKXhfZs"
   },
   "source": [
    "## Learning to localize homotypic motif density <a name='3'>\n",
    "<a href=#outline>Home</a>\n",
    "\n",
    "In this tutorial we will learn how to localize a homotypic motif cluster. We will simulate a positive set of sequences with multiple instances of a motif in the center and a negative set of sequences with multiple motif instances positioned anywhere in the sequence:\n",
    "![homotypic motif density localization](https://github.com/kundajelab/dragonn/blob/master/tutorials/tutorial_images/homotypic_motif_density_localization.jpg?raw=1)\n",
    "We will then train a binary classification model to classify the simulated sequences. To solve this task, the model will need to learn the motif pattern and whether instances of that pattern are present in the central part of the sequence.\n",
    "\n",
    "![classification task](https://github.com/kundajelab/dragonn/blob/master/tutorials/tutorial_images/homotypic_motif_density_localization_task.jpg?raw=1)\n",
    "\n",
    "We start by getting the simulation data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q5-27zcHhfZt"
   },
   "source": [
    "## Getting simulation data <a name='4'>\n",
    "<a href=#outline>Home</a>\n",
    "\n",
    "\n",
    "DragoNN provides a set of simulation functions. We will use the **simulate_motif_density_localization** function to simulate homotypic motif density localization. First, we obtain documentation for the simulation parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s9Spzg9qiqcc"
   },
   "outputs": [],
   "source": [
    "from dragonn.simulations import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 665
    },
    "colab_type": "code",
    "id": "kv-SkzqXhfZt",
    "outputId": "945efaff-fa1c-437b-a41d-d7e02c77852f"
   },
   "outputs": [],
   "source": [
    "print_simulation_info(\"simulate_motif_density_localization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0CzKV_rDhfZw"
   },
   "source": [
    "Next, we define parameters for a TAL1 motif density localization in 1500bp long sequence, with 0.4 GC fraction, and 2-4 instances of the motif in the central 150bp for the positive sequences. We simulate a total of 3000 positive and 3000 negative sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1CgzxAhShfZw"
   },
   "outputs": [],
   "source": [
    "motif_density_localization_simulation_parameters = {\n",
    "    \"motif_name\": \"TAL1_known4\",\n",
    "    \"seq_length\": 1500,\n",
    "    \"center_size\": 150,\n",
    "    \"min_motif_counts\": 2,\n",
    "    \"max_motif_counts\": 4, \n",
    "    \"num_pos\": 3000,\n",
    "    \"num_neg\": 3000,\n",
    "    \"GC_fraction\": 0.4}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3HpAcK0mhfZz"
   },
   "source": [
    "We get the simulation data by calling the **get_simulation_data** function with the simulation name and the simulation parameters as inputs. 1000 sequences are held out for a test set, 1000 sequences for a validation set, and the remaining 4000 sequences are in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y8cJCmy9hfZ0"
   },
   "outputs": [],
   "source": [
    "simulation_data = get_simulation_data(\"simulate_motif_density_localization\",\n",
    "                                      motif_density_localization_simulation_parameters,\n",
    "                                      validation_set_size=1000, test_set_size=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oJ21n_hriqc1"
   },
   "source": [
    "simulation_data provides training, validation, and test sets of input sequences X and sequence labels y. The inputs X are matrices with a one-hot-encoding of the sequences:\n",
    "<img src=\"https://github.com/kundajelab/dragonn/blob/cshl/tutorials/tutorial_images/one_hot_encoding.png?raw=1\" width=\"500\">\n",
    "\n",
    "Here are the first 10bp of a sequence in our training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NabjVH7Uiqc2",
    "outputId": "a47c5e68-b527-44eb-f8a0-6434dec6be14"
   },
   "outputs": [],
   "source": [
    "simulation_data.X_train[0, :, :10, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vLupnMwqhfZ-"
   },
   "source": [
    "We can convert this one-hot-encoded matrix back into a DNA string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8EAnH7Qziqc7",
    "outputId": "34419900-82f6-4ee4-c008-87358f996542"
   },
   "outputs": [],
   "source": [
    "simulation_data.X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "05S6ivnThfZ-",
    "outputId": "f973f351-6369-462d-e0b5-4a6e07ad7579"
   },
   "outputs": [],
   "source": [
    "from dragonn.utils import *\n",
    "get_sequence_strings(simulation_data.X_train)[0][0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DrtwHPxthfaA"
   },
   "source": [
    "Let's examine the shape of training, validation, and test matrices: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "zzNTlHQmhfaB",
    "outputId": "0e599623-7dbc-4c79-b9b6-6a578297f828"
   },
   "outputs": [],
   "source": [
    "print(simulation_data.X_train.shape)\n",
    "print(simulation_data.y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "J89jB_BFhfaE",
    "outputId": "493564bd-68f1-4feb-d632-cd9d0c727051"
   },
   "outputs": [],
   "source": [
    "print(simulation_data.X_valid.shape)\n",
    "print(simulation_data.y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "A6tHqiBEhfaH",
    "outputId": "06239636-4639-42f8-8a5f-a875d9617cd0"
   },
   "outputs": [],
   "source": [
    "print(simulation_data.X_test.shape)\n",
    "print(simulation_data.y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7MRL7Kn9iqdR"
   },
   "source": [
    "## Running dragonn on your own data: starting with FASTA files <a name='4.5'>\n",
    "<a href=#outline>Home</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aheJegthiqdS"
   },
   "source": [
    "If you are running Dragonn on your own data, you can provide data in FASTA sequence format. We recommend generating 6 fasta files for model training: \n",
    "* Training positives \n",
    "* Training negatives \n",
    "* Validation positives \n",
    "* Validation negatives \n",
    "* Test positives \n",
    "* Test negatives \n",
    "\n",
    "To indicate how this could be done, we export the one-hot-encoded matrices from **simulation_data** to a FASTA file, and then show how this fasta file could be loaded back to a one-hot-encoded matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WlJqEg__iqdU"
   },
   "outputs": [],
   "source": [
    "from dragonn.utils import fasta_from_onehot\n",
    "\n",
    "#get the indices of positive and negative sequences in the training, validation, and test sets \n",
    "train_pos=np.nonzero(simulation_data.y_train==True)\n",
    "train_neg=np.nonzero(simulation_data.y_train==False)\n",
    "valid_pos=np.nonzero(simulation_data.y_valid==True)\n",
    "valid_neg=np.nonzero(simulation_data.y_valid==False)\n",
    "test_pos=np.nonzero(simulation_data.y_test==True)\n",
    "test_neg=np.nonzero(simulation_data.y_test==False)\n",
    "\n",
    "#Generate fasta files\n",
    "fasta_from_onehot(np.expand_dims(simulation_data.X_train[train_pos],axis=1),\"X.train.pos.fasta\")\n",
    "fasta_from_onehot(np.expand_dims(simulation_data.X_valid[valid_pos],axis=1),\"X.valid.pos.fasta\")\n",
    "fasta_from_onehot(np.expand_dims(simulation_data.X_test[test_pos],axis=1),\"X.test.pos.fasta\")\n",
    "\n",
    "fasta_from_onehot(np.expand_dims(simulation_data.X_train[train_neg],axis=1),\"X.train.neg.fasta\")\n",
    "fasta_from_onehot(np.expand_dims(simulation_data.X_valid[valid_neg],axis=1),\"X.valid.neg.fasta\")\n",
    "fasta_from_onehot(np.expand_dims(simulation_data.X_test[test_neg],axis=1),\"X.test.neg.fasta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "taGDGIASiqdZ"
   },
   "source": [
    "Let's examine \"X.train.pos.fasta\" to verify that it's in the standard FASTA format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MkdRb0Eziqda",
    "outputId": "0c7bf32e-6b8b-4862-d68b-75c406fb6dcb"
   },
   "outputs": [],
   "source": [
    "!zcat X.train.pos.fasta | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_xs1nl-1iqdd"
   },
   "source": [
    "We can then load fasta format data to generate training, validation, and test splits for our models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hhtF0IPviqde"
   },
   "outputs": [],
   "source": [
    "from dragonn.utils import encode_fasta_sequences\n",
    "X_train_pos=encode_fasta_sequences(\"X.train.pos.fasta\")\n",
    "X_train_neg=encode_fasta_sequences(\"X.train.neg.fasta\")\n",
    "X_valid_pos=encode_fasta_sequences(\"X.valid.pos.fasta\")\n",
    "X_valid_neg=encode_fasta_sequences(\"X.valid.neg.fasta\")\n",
    "X_test_pos=encode_fasta_sequences(\"X.test.pos.fasta\")\n",
    "X_test_neg=encode_fasta_sequences(\"X.test.neg.fasta\")\n",
    "\n",
    "X_train=np.concatenate((X_train_pos,X_train_neg),axis=0)\n",
    "X_valid=np.concatenate((X_valid_pos,X_valid_neg),axis=0)\n",
    "X_test=np.concatenate((X_test_pos,X_test_neg),axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T2TRcn7ciqdg"
   },
   "outputs": [],
   "source": [
    "y_train=np.concatenate((np.ones(X_train_pos.shape[0]),\n",
    "                        np.zeros(X_train_neg.shape[0])))\n",
    "y_valid=np.concatenate((np.ones(X_valid_pos.shape[0]),\n",
    "                        np.zeros(X_valid_neg.shape[0])))\n",
    "y_test=np.concatenate((np.ones(X_test_pos.shape[0]),\n",
    "                        np.zeros(X_test_neg.shape[0])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aRdiNEfaiqdj"
   },
   "source": [
    "Now, having read in the FASTA files, converted them to one-hot-encoded matrices, and defined label vectors, we are ready to train our model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "giRBRUbEhfaK"
   },
   "source": [
    "# Defining the convolutional neural network model architecture  <a name='5'>\n",
    "<a href=#outline>Home</a>\n",
    "\n",
    "A locally connected linear unit in a CNN model can represent a PWM. A sequence PWM score is obtained by multiplying the PWM across the sequence, thresholding the PWM scores, and taking the max. A PWM score can also be computed by a CNN model with tiled, locally connected linear units, amounting to a convolutional layer with a single convolutional filter representing the PWM, followed by ReLU thresholding and maxpooling\n",
    "![dragonn vs pssm](https://github.com/kundajelab/dragonn/blob/master/tutorials/tutorial_images/dragonn_and_pssm.jpg?raw=1)\n",
    "By utilizing multiple convolutional layers with multiple convolutional filters, CNN's can represent a wide range of sequence features in a compositional fashion:\n",
    "![dragonn model figure](https://github.com/kundajelab/dragonn/blob/cshl/tutorials/tutorial_images/dragonn_model_figure.png?raw=1)\n",
    "\n",
    "We will use the deep learning library [keras](http://keras.io/) with the [TensorFlow](https://github.com/tensorflow/tensorflow) backend to generate and train the CNN models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BMEgqp-bhfaL",
    "outputId": "3b6b884a-2b97-4ca1-e64d-313b8fb0543f"
   },
   "outputs": [],
   "source": [
    "#To prepare for model training, we import the necessary functions and submodules from keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Reshape, Dense, Activation, Flatten,Conv2D, MaxPooling2D,BatchNormalization\n",
    "# from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "# from keras.optimizers import Adadelta, SGD, RMSprop;\n",
    "import keras.losses;\n",
    "from keras.constraints import maxnorm;\n",
    "# from keras.layers.normalization import BatchNormalization\n",
    "from keras.regularizers import l1, l2\n",
    "from keras.callbacks import EarlyStopping, History\n",
    "from keras import backend as K \n",
    "K.set_image_data_format('channels_last')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A multi-layer DragoNN model <a name='9'>\n",
    "\n",
    "<a href=#outline>Home</a> \n",
    "\n",
    "Next, we train a 3 layer model for this task. Will it outperform the single layer model and to what extent will it overfit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the model architecture in keras\n",
    "multi_layer_keras_model=Sequential() \n",
    "multi_layer_keras_model.add(Conv2D(filters=15,kernel_size=(1,10),input_shape=simulation_data.X_train.shape[1::]))\n",
    "multi_layer_keras_model.add(Activation('relu'))\n",
    "\n",
    "multi_layer_keras_model.add(Conv2D(filters=15,kernel_size=(1,10),input_shape=simulation_data.X_train.shape[1::]))\n",
    "multi_layer_keras_model.add(Activation('relu'))\n",
    "\n",
    "multi_layer_keras_model.add(Conv2D(filters=15,kernel_size=(1,10),input_shape=simulation_data.X_train.shape[1::]))\n",
    "multi_layer_keras_model.add(Activation('relu'))\n",
    "multi_layer_keras_model.add(MaxPooling2D(pool_size=(1,35)))\n",
    "\n",
    "\n",
    "multi_layer_keras_model.add(Flatten())\n",
    "multi_layer_keras_model.add(Dense(1))\n",
    "multi_layer_keras_model.add(Activation(\"sigmoid\"))\n",
    "\n",
    "##compile the model, specifying the Adam optimizer, and binary cross-entropy loss. \n",
    "multi_layer_keras_model.compile(optimizer='adam',\n",
    "                               loss='binary_crossentropy')\n",
    "\n",
    "multi_layer_keras_model.summary() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the model for 150 epochs, with an early stopping criterion -- if the loss on the validation set does not improve for five consecutive epochs, the training is halted. In each epoch, the one_filter_dragonn performed a complete pass over the training data, and updated its parameters to minimize the loss, which quantifies the error in the model predictions. After each epoch, the performance metrics for the one_filter_dragonn on the validation data were stored. \n",
    "\n",
    "The performance metrics include balanced accuracy, area under the receiver-operating curve ([auROC](https://en.wikipedia.org/wiki/Receiver_operating_characteristic)), are under the precision-recall curve ([auPRC](https://en.wikipedia.org/wiki/Precision_and_recall)), and recall for multiple false discovery rates  (Recall at [FDR](https://en.wikipedia.org/wiki/False_discovery_rate))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dragonn.callbacks import * \n",
    "#We define a custom callback to print training and validation metrics while training. \n",
    "metrics_callback=MetricsCallback(train_data=(simulation_data.X_train,simulation_data.y_train),\n",
    "                                 validation_data=(simulation_data.X_valid,simulation_data.y_valid))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## use the keras fit function to train the model for 150 epochs with early stopping after 3 epochs \n",
    "history_multi_layer=multi_layer_keras_model.fit(x=simulation_data.X_train,\n",
    "                                  y=simulation_data.y_train,\n",
    "                                  batch_size=128,\n",
    "                                  epochs=150,\n",
    "                                  verbose=1,\n",
    "                                  callbacks=[metrics_callback,\n",
    "                                      EarlyStopping(patience=3,restore_best_weights=True)],\n",
    "                                  validation_data=(simulation_data.X_valid,\n",
    "                                                   simulation_data.y_valid))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model on the held-out test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use the keras predict function to get model predictions on held-out test set. \n",
    "test_predictions=multi_layer_keras_model.predict(simulation_data.X_test)\n",
    "## Generate a ClassificationResult object to print performance metrics on held-out test set \n",
    "print(ClassificationResult(simulation_data.y_test,test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the model's performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the validation loss is not decreasing and the auROC metric is not decreasing, which indicates this model is not learning. A simple plot of the learning curve, showing the loss function on the training and validation data over the course of training, demonstrates this visually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "#import functions foro visualization of data \n",
    "from dragonn.vis import *\n",
    "## Visualize the model's performance \n",
    "plot_learning_curve(history_multi_layer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model performs slightly better than the single layer model but it overfits more. We will try to address that with dropout regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We export the new model to hdf5 to allow for subsequent interpretation with Interpretation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_layer_keras_model.save(\"tut1_multi_layers_keras_model.hdf5\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now load the saved model for use in other applications or for further fine-tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "multi_layer_keras_model=load_model(\"tut1_multi_layers_keras_model.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualize the motifs learned by the model\n",
    "%matplotlib inline\n",
    "plot_filters(multi_layer_keras_model, simulation_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WHsr3OughfbB"
   },
   "source": [
    "# Model Interpretation <a name='8'>\n",
    "<a href=#outline>Home</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8n_EMffXhfbC"
   },
   "source": [
    "As you can see, the filters/model parameters are difficult to be interepreted directly. However, there are alternative approaches of interepreting sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RzPdGqgDhfbD"
   },
   "source": [
    "Let's examine a positive and negative example from our simulation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5Spz3zpGhfbE",
    "outputId": "843855d0-51d6-4098-8340-ae08c1fc2886"
   },
   "outputs": [],
   "source": [
    "#get the indices of the first positive and negative examples in the validation data split\n",
    "pos_indx=np.flatnonzero(simulation_data.y_valid==1)[0]\n",
    "print(pos_indx)\n",
    "pos_X=simulation_data.X_valid[pos_indx:pos_indx+1]\n",
    "\n",
    "neg_indx=np.flatnonzero(simulation_data.y_valid==0)[10]\n",
    "print(neg_indx)\n",
    "neg_X=simulation_data.X_valid[neg_indx:neg_indx+1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rnZ939dqhfbJ"
   },
   "source": [
    "### Motif Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2ESoHCt9hfbJ"
   },
   "outputs": [],
   "source": [
    "from dragonn.utils import * \n",
    "pos_motif_scores=get_motif_scores(pos_X,simulation_data.motif_names,return_positions=True)\n",
    "neg_motif_scores=get_motif_scores(neg_X,simulation_data.motif_names,return_positions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tAsR6VhphfbL",
    "outputId": "3a295714-45b1-4264-e6eb-ecc647bbf789"
   },
   "outputs": [],
   "source": [
    "from dragonn.vis import * \n",
    "plot_motif_scores(pos_motif_scores,title=\"Positive example\",ylim=(0,20))\n",
    "plot_motif_scores(neg_motif_scores,title=\"Negative example\",ylim=(0,20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yYLxiOEShfbP"
   },
   "source": [
    "The motif scan yields a group of three high-scoring motif alignment positions at a fixed distance near the center of the sequence in the positive example. The spacing of the high-scoring motif alignments is random in the negative sequence. \n",
    "\n",
    "Note: If you find that your negative example is too close to the positive examle (i.e. the randomly spaced motifs happen to have a spacing close to the positive example, feel free to provide another index value to select a different negative). \n",
    "\n",
    "For example, you can change the code to select a negative example to the below: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fxVLJFtjiqfA",
    "outputId": "82e359e0-182e-4837-950a-2e64cc3d97f8"
   },
   "outputs": [],
   "source": [
    "neg_indx=np.flatnonzero(simulation_data.y_valid==0)[10]\n",
    "print(neg_indx)\n",
    "neg_X=simulation_data.X_valid[neg_indx:neg_indx+1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RCfzueaWhfbQ"
   },
   "source": [
    "### *In silico* mutagenesis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tAxE_amLhfbQ"
   },
   "source": [
    "To determine how much each position in the input sequence contrinbutes to the model's prediction, we can perform saturation mutagenesis on the sequence. For each position in the input sequence, we introduce each of the four possible bases A, C, G, T and quantify the effect on the model's predictions.\n",
    "\n",
    "*In silico* mutagenesis entails measuring the effect of a given base pair on the model's prediction of accessibility. The following algorithm is used: \n",
    "\n",
    "1. At each position in the input sequence, the reference allele is mutated to each of three possible alternate alleles, and the model predictions with the alternate alleles are obtained. \n",
    "\n",
    "2. The logit values for the reference allele are subtracted from the logit values for each of the 4 alleles. (This means that a difference of 0 will be obtained for the reference allele). We refer to these differences in logit at each position between the reference and alternate alleles as the ISM values. ISM values are computed in logit space to avoid any saturation effects from passing the logits through a sigmoid function. \n",
    "\n",
    "3. For each position, subtract the mean ISM value for that position from each of the 4 ISM values. \n",
    "\n",
    "4. Plot the 4xL heatmap of mean-normalized ISM values \n",
    "\n",
    "5. Identity the highest scoring (mean subtracted) allele at each position. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8Cod4ryQiqfE",
    "outputId": "9e5b9027-af6f-4561-cf42-46643bdbfc97"
   },
   "outputs": [],
   "source": [
    "pos_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e-LN7IlUhfbQ",
    "outputId": "9b598d5b-d0f9-4f65-c689-e4a89a4fb6d6"
   },
   "outputs": [],
   "source": [
    "from dragonn.interpret.ism import *\n",
    "ism_pos=in_silico_mutagenesis(multi_layer_keras_model,pos_X,0)\n",
    "ism_neg=in_silico_mutagenesis(multi_layer_keras_model,neg_X,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l6Vya1NBhfbT",
    "outputId": "2c29d553-077c-40c0-9301-84e60547642b",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from dragonn.vis import * \n",
    "# create discrete colormap of ISM scores \n",
    "plot_ism(ism_pos,pos_X,title=\"Positive Example\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KsUgcZT2iqfL",
    "outputId": "2e254fd3-9ac6-4494-9e1d-99a93d9cfd3e"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plot_ism(ism_neg,neg_X,title=\"Negative Example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LElrqg4hhfbX"
   },
   "source": [
    "### Gradient x Input "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sToN6KaqhfbX"
   },
   "source": [
    "Consider a neural net being a function: $f(x_1, ..., x_N; w) = y$\n",
    "\n",
    "One way to tell whether the input feature is important is to compute the gradient of the function with respect to (w.r.t.) model input: $\\frac{\\partial f}{\\partial x_i}$\n",
    "\n",
    "This approach is called saliency maps: \"Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps\", by Karen Simonyan, Andrea Vedaldi and Andrew Zisserma https://arxiv.org/pdf/1312.6034.pdf\n",
    "\n",
    "In genomics, we typically visualize only gradients for bases observed in the sequence (called input masked gradients or input*grad)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Muro8H0Ahfba",
    "outputId": "da28f53d-9aa8-459f-8df8-2a05e6cee732"
   },
   "outputs": [],
   "source": [
    "from dragonn.interpret.input_grad import * \n",
    "gradinput_pos=input_grad(multi_layer_keras_model,pos_X)\n",
    "gradinput_neg=input_grad(multi_layer_keras_model,neg_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0p64eg1Ghfbb",
    "outputId": "113f6a69-a5d1-4813-803b-47b6011ab2fa"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from dragonn.vis import plot_seq_importance\n",
    "plot_seq_importance(gradinput_pos,pos_X,title=\"Positive GradXInput\")\n",
    "plot_seq_importance(gradinput_neg,neg_X,title=\"Negative GradXInput\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M1_eQsEshfbg"
   },
   "source": [
    "Let's zoom in to the center 150 bp of the sequence, where the simulated homotypic motif grammar is to be found. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_VbnG8C6hfbg",
    "outputId": "2244146a-4d85-4952-b2be-70bef8ef4b87"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plot_seq_importance(gradinput_pos,pos_X,title=\"Positive GradXInput\",xlim=(675,825))\n",
    "plot_seq_importance(gradinput_neg,neg_X,title=\"Negative GradXInput\",xlim=(675,825))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Votf6uXthfbl"
   },
   "source": [
    "### DeepShap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9-V9EmMqhfbl"
   },
   "source": [
    "[DeepShap](https://arxiv.org/pdf/1705.07874.pdf) computes the shapley value for every feature. The Shapley value is the average contribution of a feature to the prediction in different  possible combinations (coalitions). \n",
    "\n",
    "DeepShap can accept a custom reference. For our purposes, we provide a dinucleotide-shuffled reference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dragonn.interpret.deepshap import * \n",
    "shap_scores_pos = deep_shap(multi_layer_keras_model,pos_X)\n",
    "shap_scores_neg = deep_shap(multi_layer_keras_model,neg_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plot_seq_importance(shap_scores_pos,pos_X,title=\"DeepShap positives\")\n",
    "plot_seq_importance(shap_scores_neg,neg_X,title=\"DeepShap negatives\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plot_seq_importance(shap_scores_pos,pos_X,title=\"DeepShap positives\",xlim=(675,825))\n",
    "plot_seq_importance(shap_scores_neg,neg_X,title=\"DeepShap negatives\",xlim=(675,825))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lJ_CcYdohfcL"
   },
   "source": [
    "# A regularized multi-layer DragoNN model <a name='10'>\n",
    "    \n",
    "<a href=#outline>Home</a> \n",
    "    \n",
    "Next, we regularize the 3 layer using 0.2 dropout on every convolutional layer. Will dropout improve validation performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d6nxUApfhfcL",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Define the model architecture in keras\n",
    "\n",
    "regularized_keras_model=Sequential() \n",
    "regularized_keras_model.add(Conv2D(filters=15,kernel_size=(1,10),input_shape=simulation_data.X_train.shape[1::]))\n",
    "regularized_keras_model.add(Activation('relu'))\n",
    "regularized_keras_model.add(Dropout(0.2))\n",
    "\n",
    "regularized_keras_model.add(Conv2D(filters=15,kernel_size=(1,10),input_shape=simulation_data.X_train.shape[1::]))\n",
    "regularized_keras_model.add(Activation('relu'))\n",
    "regularized_keras_model.add(Dropout(0.2))\n",
    "\n",
    "regularized_keras_model.add(Conv2D(filters=15,kernel_size=(1,10),input_shape=simulation_data.X_train.shape[1::]))\n",
    "regularized_keras_model.add(Activation('relu'))\n",
    "regularized_keras_model.add(Dropout(0.2))\n",
    "regularized_keras_model.add(MaxPooling2D(pool_size=(1,35)))\n",
    "\n",
    "\n",
    "regularized_keras_model.add(Flatten())\n",
    "regularized_keras_model.add(Dense(1))\n",
    "regularized_keras_model.add(Activation(\"sigmoid\"))\n",
    "\n",
    "##compile the model, specifying the Adam optimizer, and binary cross-entropy loss. \n",
    "regularized_keras_model.compile(optimizer='adam',\n",
    "                               loss='binary_crossentropy')\n",
    "\n",
    "regularized_keras_model.summary() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NL7Ivww6hfcN",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## use the keras fit function to train the model for 150 epochs with early stopping after 3 epochs \n",
    "history_regularized=regularized_keras_model.fit(x=simulation_data.X_train,\n",
    "                                  y=simulation_data.y_train,\n",
    "                                  batch_size=128,\n",
    "                                  epochs=150,\n",
    "                                  verbose=1,\n",
    "                                  callbacks=[metrics_callback,\n",
    "                                            EarlyStopping(patience=3,restore_best_weights=True),],\n",
    "                                  validation_data=(simulation_data.X_valid,\n",
    "                                                   simulation_data.y_valid))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NB0ockF6hfcQ"
   },
   "outputs": [],
   "source": [
    "## Use the keras predict function to get model predictions on held-out test set. \n",
    "test_predictions=regularized_keras_model.predict(simulation_data.X_test)\n",
    "## Generate a ClassificationResult object to print performance metrics on held-out test set \n",
    "print(ClassificationResult(simulation_data.y_test,test_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q4LBbu66hfcS"
   },
   "outputs": [],
   "source": [
    "## Visualize the model's performance \n",
    "plot_learning_curve(history_regularized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RXof_1ZBiqgO"
   },
   "source": [
    "### Interpreting regularized, multi-layer model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5Zz2ehvoiqgP"
   },
   "outputs": [],
   "source": [
    "regularized_keras_model.save(\"TAL1.Simulation.Regularized.3ConvLayers.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "regularized_keras_model=load_model(\"TAL1.Simulation.Regularized.3ConvLayers.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rAVUWCeZiqgQ"
   },
   "outputs": [],
   "source": [
    "from dragonn.interpret import *\n",
    "pos_interpretations=multi_method_interpret(regularized_keras_model,\n",
    "                                           pos_X,\n",
    "                                           0,\n",
    "                                           motif_names=simulation_data.motif_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cLUn92BAiqgR"
   },
   "outputs": [],
   "source": [
    "neg_interpretations=multi_method_interpret(regularized_keras_model,\n",
    "                                           neg_X,\n",
    "                                           0,\n",
    "                                           motif_names=simulation_data.motif_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "scTNvahxiqgU"
   },
   "source": [
    "We now plot the interpretation scores for pos_X and neg_X along the full sequence as well as along the central 200 bp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4iLt_YfQiqgU"
   },
   "outputs": [],
   "source": [
    "plot_all_interpretations([pos_interpretations],pos_X,xlim=(650,850))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5QQ09gJeiqgV"
   },
   "outputs": [],
   "source": [
    "plot_all_interpretations([neg_interpretations],neg_X,xlim=(650,850))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, dropout decreased the overfitting this model displayed previously and increased test performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ys1cqU5whfce"
   },
   "source": [
    "## For further exploration<a name='11'>\n",
    "<a href=#outline>Home</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TRjJ5t00hfce"
   },
   "source": [
    "In this tutorial we explored modeling of homotypic motif density. Other properties of regulatory DNA sequence include\n",
    "![sequence properties 3](https://github.com/kundajelab/dragonn/blob/master/tutorials/tutorial_images/sequence_properties_3.jpg?raw=1)\n",
    "![sequence properties 4](https://github.com/kundajelab/dragonn/blob/master/tutorials/tutorial_images/sequence_properties_4.jpg?raw=1)\n",
    "\n",
    "DragoNN provides simulations that formulate learning these patterns into classification problems:\n",
    "![sequence](https://github.com/kundajelab/dragonn/blob/master/tutorials/tutorial_images/sequence_simulations.png?raw=1)\n",
    "\n",
    "You can view the available simulation functions by running print_available_simulations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6BLPWVdhhfcf"
   },
   "outputs": [],
   "source": [
    "print_available_simulations()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "PrimerTutorial 2 - Interpreting features induced by CNN's",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "dragonn",
   "language": "python",
   "name": "dragonn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
