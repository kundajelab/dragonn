{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PrimerTutorial 1 - Exploring model architectures for a homotypic motif density simulation.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APNfq3knhfZg"
      },
      "source": [
        "# How to train your DragoNN: \n",
        "## Exploring convolutional neural network (CNN) architectures for simulated genomic data as well as ENCODE TF ChIP-seq datasets. \n",
        "\n",
        "This tutorial will take approximately 2 hours if executed on a GPU. \n",
        "\n",
        "## Outline<a name='outline'>\n",
        "<ol>\n",
        "    <li><a href=#1>How to use this tutorial</a></li>\n",
        "    <li><a href=#2>Review of patterns in transcription factor binding sites</a></li>\n",
        "    <li><a href=#3>Learning to localize homotypic motif density</a></li>\n",
        "    <li><a href=#4>Simulate training data with simdna</a></li>  \n",
        "    <li><a href=#4.5>Running dragonn on your own data: starting with FASTA files</a></li>\n",
        "    <li><a href=#5>Defining CNN architecture</a></li>\n",
        "    <li><a href=#6>Single layer, multiple filter model</a></li>\n",
        "    <li><a href=#7>Model Interpretation</a></li>    \n",
        "    <li><a href=#8>Regularized multi-layer model</a></li>\n",
        "    <li><a href=#9>Training on real data: SPI1 ChIP-seq and experimental bQTL data</a></li>\n",
        "    <li><a href=#10>Genomewide classification and regression labels for SPI1 TF ChIP-seq</a></li>\n",
        "    <li><a href=#11>Genome-wide classification for SPI1</a></li>\n",
        "    <li><a href=#12>Genome-wide regression for SPI1</a></li> \n",
        "    <li><a href=#13>Genome-wide interpretation of true positive predictions in SPI1</a></li>\n",
        "    <li><a href=#14>Applications for SPI1 bQTL dataset</a></li>\n",
        "    <ol>\n",
        "        <li><a href=#a>Read in and annotate SPI1 bQTL dataset</a></li>\n",
        "        <li><a href=#b>Obatin DNN predictions for POST and ALT alleles within the bQTL dataset</a></li>\n",
        "        <li><a href=#c>Compare model predictions on POST and ALT alleles </a></li>\n",
        "        <li><a href=#d>bQTL dataset motif scan with HOCOMOCO SPI1 motif </a></li>\n",
        "        <li><a href=#e>bQTL interpretation summary: deep learning models vs motif scan</a></li>\n",
        "    </ol>\n",
        "    <li><a href=#15>Deep learning models are able to identify low-affinity TF-binding sites missed by motif scanning.</a></li>\n",
        "</ol>\n",
        "    \n",
        "Github issues on the [dragonn repository](https://github.com/kundajelab/dragonn) with feedback, questions, and discussion are always welcome.\n",
        "\n",
        "Github issues on the dragonn repository with feedback, questions, and discussion are always welcome.\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NixF5bW3hfZg"
      },
      "source": [
        "## How to use this tutorial<a name='1'>\n",
        "<a href=#outline>Home</a>\n",
        "\n",
        "This tutorial utilizes a Google Colaboratory Notebook - an interactive computational enviroment that combines live code, visualizations, and explanatory text. The notebook is organized into a series of cells. \n",
        "\n",
        "The first thing we do is set our Runtime to use Python3 and GPU. \n",
        "\n",
        "![ChangeRuntime](https://github.com/kundajelab/dragonn/blob/master/tutorials/tutorial_images/ChangeRuntime.png?raw=true)\n",
        "\n",
        "![RuntimeType.png](https://github.com/kundajelab/dragonn/blob/master/tutorials/tutorial_images/RuntimeType.png?raw=1)\n",
        "\n",
        "Now that we set our Runtime, we can execute the cells in the notebook. You can execute the cells one at a time by clicking inside of them and pressing SHIFT+enter. Alternatively, you can run all the cells by clicking the \"Run All\" button, as demonstrated below. \n",
        "\n",
        "![RunAllColab](https://github.com/kundajelab/dragonn/blob/master/tutorials/tutorial_images/RunAllCollab.png?raw=1)\n",
        "\n",
        "\n",
        "You can run the next cell by cliking the play button:\n",
        "\n",
        "![RunCellArrow](https://github.com/kundajelab/dragonn/blob/master/tutorials/tutorial_images/RunCellArrow.png?raw=1)\n",
        "\n",
        "Half of the cells in this tutorial contain code, the other half contain visualizations and explanatory text. Code, visualizations, and text in cells can be modified - you are encouraged to modify the code as you advance through the tutorial. You can inspect the implementation of a function used in a cell by following these steps:\n",
        "\n",
        "![inspecting code](https://github.com/kundajelab/dragonn/blob/master/tutorials/tutorial_images/inspecting_code.png?raw=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWLcVFDXzihK"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wORsai47hfZi"
      },
      "source": [
        "#uncomment the lines below if you are running this tutorial from Google Colab \n",
        "!pip uninstall albumentations -y \n",
        "!pip uninstall folium -y \n",
        "!pip uninstall datascience -y \n",
        "!pip install dragonn==0.4.1 \n",
        "!pip uninstall tensorflow \n",
        "!pip install tensorflow-gpu==1.15\n",
        "!pip install keras==2.3 \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSaW2fD2hfZk"
      },
      "source": [
        "# Making sure our results are reproducible\n",
        "from numpy.random import seed\n",
        "seed(1234)\n",
        "import tensorflow as tf\n",
        "#import tensorflow.compat.v1 as tf\n",
        "#tf.disable_v2_behavior() \n",
        "tf.set_random_seed(1234)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-bvAg5-hfZn"
      },
      "source": [
        "We start by loading dragonn's tutorial utilities and reviewing properties of regulatory sequence that transcription factors bind."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1e2J6BZ-hfZo"
      },
      "source": [
        "#load dragonn tutorial utilities \n",
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7e3bYhMBhfZr"
      },
      "source": [
        "## Key properties of regulatory DNA sequences <a name='2'>\n",
        "<a href=#outline>Home</a>\n",
        "\n",
        "![sequence properties 1](https://github.com/kundajelab/dragonn/blob/master/paper_supplement/primer_tutorial_images/sequence_properties_1.jpg?raw=1)\n",
        "![sequence properties 2](https://github.com/kundajelab/dragonn/blob/master/paper_supplement/primer_tutorial_images/sequence_properties_2.jpg?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ga1AXeKXhfZs"
      },
      "source": [
        "## Learning to localize homotypic motif density <a name='3'>\n",
        "<a href=#outline>Home</a>\n",
        "\n",
        "In this tutorial we will learn how to localize a homotypic motif cluster. We will simulate a positive set of sequences with multiple instances of a motif in the center and a negative set of sequences with multiple motif instances positioned anywhere in the sequence:\n",
        "![homotypic motif density localization](https://github.com/kundajelab/dragonn/blob/master/tutorials/tutorial_images/homotypic_motif_density_localization.jpg?raw=1)\n",
        "We will then train a binary classification model to classify the simulated sequences. To solve this task, the model will need to learn the motif pattern and whether instances of that pattern are present in the central part of the sequence.\n",
        "\n",
        "![classification task](https://github.com/kundajelab/dragonn/blob/master/tutorials/tutorial_images/homotypic_motif_density_localization_task.jpg?raw=1)\n",
        "\n",
        "We start by getting the simulation data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5-27zcHhfZt"
      },
      "source": [
        "## Getting simulation data <a name='4'>\n",
        "<a href=#outline>Home</a>\n",
        "\n",
        "\n",
        "DragoNN provides a set of simulation functions. We will use the **simulate_motif_density_localization** function to simulate homotypic motif density localization. First, we obtain documentation for the simulation parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwUscf6NzihY"
      },
      "source": [
        "from dragonn.simulations import * "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kv-SkzqXhfZt"
      },
      "source": [
        "print_simulation_info(\"simulate_motif_density_localization\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0CzKV_rDhfZw"
      },
      "source": [
        "Next, we define parameters for a TAL1 motif density localization in 1500bp long sequence, with 0.4 GC fraction, and 2-4 instances of the motif in the central 150bp for the positive sequences. We simulate a total of 3000 positive and 3000 negative sequences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1CgzxAhShfZw"
      },
      "source": [
        "motif_density_localization_simulation_parameters = {\n",
        "    \"motif_name\": \"TAL1_known4\",\n",
        "    \"seq_length\": 1500,\n",
        "    \"center_size\": 150,\n",
        "    \"min_motif_counts\": 2,\n",
        "    \"max_motif_counts\": 4, \n",
        "    \"num_pos\": 3000,\n",
        "    \"num_neg\": 3000,\n",
        "    \"GC_fraction\": 0.4}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HpAcK0mhfZz"
      },
      "source": [
        "We get the simulation data by calling the **get_simulation_data** function with the simulation name and the simulation parameters as inputs. 1000 sequences are held out for a test set, 1000 sequences for a validation set, and the remaining 4000 sequences are in the training set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8cJCmy9hfZ0"
      },
      "source": [
        "simulation_data = get_simulation_data(\"simulate_motif_density_localization\",\n",
        "                                      motif_density_localization_simulation_parameters,\n",
        "                                      validation_set_size=1000, test_set_size=1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-e-3Sk0Azihm"
      },
      "source": [
        "simulation_data provides training, validation, and test sets of input sequences X and sequence labels y. The inputs X are matrices with a one-hot-encoding of the sequences:\n",
        "<img src=\"https://github.com/kundajelab/dragonn/blob/master/tutorials/tutorial_images/one_hot_encoding.png?raw=1\" width=\"500\">\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8fJ0c1Mzihm"
      },
      "source": [
        "Simulation data is an object. It contains an attribute called X_train that is a numpy array of 4 dimensions. We can call the \"shape\" function on X_train to get it's dimensions. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EuEbg6Ftzihn"
      },
      "source": [
        "simulation_data.X_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McjA51Uk2ehG"
      },
      "source": [
        "Here are the first 10bp of a sequence in our training data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uGOwNHTziht"
      },
      "source": [
        "#The first dimension indicates the index of the training samples. \n",
        "# The second dimension is 1, and is only necessary because we are \n",
        "# performing 2D convolutions. We could omit this \"dummy\" dimension if\n",
        "# we used 1D convolutions. \n",
        "# The third dimension indicates the base index. \n",
        "# The fourth dimension indicates the base pair channels: A,C,G,T. \n",
        "\n",
        "simulation_data.X_train[0, :, :10, :]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLupnMwqhfZ-"
      },
      "source": [
        "We can convert this one-hot-encoded matrix back into a DNA string:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05S6ivnThfZ-"
      },
      "source": [
        "from dragonn.utils import *\n",
        "get_sequence_strings(simulation_data.X_train)[0][0:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DrtwHPxthfaA"
      },
      "source": [
        "Let's examine the shape of training, validation, and test matrices: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzNTlHQmhfaB"
      },
      "source": [
        "print(simulation_data.X_train.shape)\n",
        "print(simulation_data.y_train.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J89jB_BFhfaE"
      },
      "source": [
        "print(simulation_data.X_valid.shape)\n",
        "print(simulation_data.y_valid.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6tHqiBEhfaH"
      },
      "source": [
        "print(simulation_data.X_test.shape)\n",
        "print(simulation_data.y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zSjm7LWziiG"
      },
      "source": [
        "## Running dragonn on your own data: starting with FASTA files <a name='4.5'>\n",
        "<a href=#outline>Home</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2G6qRmZziiH"
      },
      "source": [
        "If you are running Dragonn on your own data, you can provide data in FASTA sequence format. We recommend generating 6 fasta files for model training: \n",
        "* Training positives \n",
        "* Training negatives \n",
        "* Validation positives \n",
        "* Validation negatives \n",
        "* Test positives \n",
        "* Test negatives \n",
        "\n",
        "To indicate how this could be done, we export the one-hot-encoded matrices from **simulation_data** to a FASTA file, and then show how this fasta file could be loaded back to a one-hot-encoded matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GsA3TIC0ziiI"
      },
      "source": [
        "from dragonn.utils import fasta_from_onehot\n",
        "\n",
        "#get the indices of positive and negative sequences in the training, validation, and test sets \n",
        "train_pos=np.nonzero(simulation_data.y_train==True)\n",
        "train_neg=np.nonzero(simulation_data.y_train==False)\n",
        "valid_pos=np.nonzero(simulation_data.y_valid==True)\n",
        "valid_neg=np.nonzero(simulation_data.y_valid==False)\n",
        "test_pos=np.nonzero(simulation_data.y_test==True)\n",
        "test_neg=np.nonzero(simulation_data.y_test==False)\n",
        "\n",
        "#Generate gzipped  fasta files -- it is always a good idea to gzip your fasta files. This is less \n",
        "# important for our tiny example files, but becomes more relevant as the size of the files increases. \n",
        "# The fasta_from_onehot function gzips output fasta files. \n",
        "fasta_from_onehot(np.expand_dims(simulation_data.X_train[train_pos],axis=1),\"X.train.pos.fasta.gz\")\n",
        "fasta_from_onehot(np.expand_dims(simulation_data.X_valid[valid_pos],axis=1),\"X.valid.pos.fasta.gz\")\n",
        "fasta_from_onehot(np.expand_dims(simulation_data.X_test[test_pos],axis=1),\"X.test.pos.fasta.gz\")\n",
        "\n",
        "fasta_from_onehot(np.expand_dims(simulation_data.X_train[train_neg],axis=1),\"X.train.neg.fasta.gz\")\n",
        "fasta_from_onehot(np.expand_dims(simulation_data.X_valid[valid_neg],axis=1),\"X.valid.neg.fasta.gz\")\n",
        "fasta_from_onehot(np.expand_dims(simulation_data.X_test[test_neg],axis=1),\"X.test.neg.fasta.gz\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tNuyVpaziiK"
      },
      "source": [
        "Let's examine \"X.train.pos.fasta.gz\" to verify that it's in the standard gzipped FASTA format. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f16lnk8cziiK"
      },
      "source": [
        "! zcat X.train.pos.fasta.gz | head"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGFUpDVPziiO"
      },
      "source": [
        "We can then load fasta format data to generate training, validation, and test splits for our models:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2F1CWZ8iziiO"
      },
      "source": [
        "from dragonn.utils import encode_fasta_sequences\n",
        "X_train_pos=encode_fasta_sequences(\"X.train.pos.fasta.gz\")\n",
        "X_train_neg=encode_fasta_sequences(\"X.train.neg.fasta.gz\")\n",
        "X_valid_pos=encode_fasta_sequences(\"X.valid.pos.fasta.gz\")\n",
        "X_valid_neg=encode_fasta_sequences(\"X.valid.neg.fasta.gz\")\n",
        "X_test_pos=encode_fasta_sequences(\"X.test.pos.fasta.gz\")\n",
        "X_test_neg=encode_fasta_sequences(\"X.test.neg.fasta.gz\")\n",
        "\n",
        "X_train=np.concatenate((X_train_pos,X_train_neg),axis=0)\n",
        "X_valid=np.concatenate((X_valid_pos,X_valid_neg),axis=0)\n",
        "X_test=np.concatenate((X_test_pos,X_test_neg),axis=0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcSZAyGdziiQ"
      },
      "source": [
        "y_train=np.concatenate((np.ones(X_train_pos.shape[0]),\n",
        "                        np.zeros(X_train_neg.shape[0])))\n",
        "y_valid=np.concatenate((np.ones(X_valid_pos.shape[0]),\n",
        "                        np.zeros(X_valid_neg.shape[0])))\n",
        "y_test=np.concatenate((np.ones(X_test_pos.shape[0]),\n",
        "                        np.zeros(X_test_neg.shape[0])))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYOwbRPqziiS"
      },
      "source": [
        "Now, having read in the FASTA files, converted them to one-hot-encoded matrices, and defined label vectors, we are ready to train our model. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "giRBRUbEhfaK"
      },
      "source": [
        "# Defining the convolutional neural network model architecture  <a name='5'>\n",
        "<a href=#outline>Home</a>\n",
        "\n",
        "A locally connected linear unit in a CNN model can represent a PSSM (part a). A sequence PSSM score is obtained by multiplying the PSSM across the sequence, thresholding the PSSM scores, and taking the max (part b). A PSSM score can also be computed by a CNN model with tiled, locally connected linear units, amounting to a convolutional layer with a single convolutional filter representing the PSSM, followed by ReLU thresholding and maxpooling (part c).\n",
        "![dragonn vs pssm](https://github.com/kundajelab/dragonn/blob/master/tutorials/tutorial_images/dragonn_and_pssm.jpg?raw=1)\n",
        "\n",
        "\n",
        "By utilizing multiple convolutional layers with multiple convolutional filters, CNN's can represent a wide range of sequence features in a compositional fashion:\n",
        "![dragonn model figure](https://github.com/kundajelab/dragonn/blob/master/tutorials/tutorial_images/dragonn_model_figure.jpg?raw=1)\n",
        "\n",
        "\n",
        "We will use the deep learning library [keras](http://keras.io/) with the [TensorFlow](https://github.com/tensorflow/tensorflow) backend to generate and train the CNN models. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMEgqp-bhfaL"
      },
      "source": [
        "#To prepare for model training, we import the necessary functions and submodules from keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dropout, Reshape, Dense, Activation, Flatten\n",
        "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
        "from keras.optimizers import Adadelta, SGD, RMSprop;\n",
        "import keras.losses;\n",
        "from keras.constraints import maxnorm;\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.regularizers import l1, l2\n",
        "from keras.callbacks import EarlyStopping, History\n",
        "from keras import backend as K \n",
        "K.set_image_data_format('channels_last')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80GBS3CFhfaN"
      },
      "source": [
        "# Single layer, multi-filter model <a name='6'>\n",
        "<a href=#outline>Home</a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfS72LrxhfaN"
      },
      "source": [
        "We define a simple DragoNN model with one convolutional layer with 15 convolutional filters, followed by maxpooling of width 35. \n",
        "\n",
        "The model parameters are: \n",
        "\n",
        "* Input sequence length 1500 \n",
        "* 15 filter: there are neurons that act as  local pattern detectors on the input profile. \n",
        "* Convolutional filter width =  10: this metric defines the dimension of the filter weights; the model scans the entire input profile for a particular pattern encoded by the weights of the filter. \n",
        "* Max pool of width 35: computes the maximum value per-channel in sliding windows of size 35. We add the pooling layer becase DNA sequences are typically sparse in terms of the number of positions in the sequence that harbor TF motifs. The pooling layer allows us to reduce the size of the output profile of convolutional layers by employing summary statistics. \n",
        "\n",
        "![simArch1Layer](https://github.com/kundajelab/dragonn/blob/master/tutorials/tutorial_images/SimArch1Layer.png?raw=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3oBUEZ7wMJkq"
      },
      "source": [
        "!pip show tensorflow-gpu \n",
        "!pip show keras \n",
        "!pip show dragonn "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ikMRDXRhfaO"
      },
      "source": [
        "#Define the model architecture in keras\n",
        "multi_filter_keras_model=Sequential() \n",
        "multi_filter_keras_model.add(Conv2D(filters=15,kernel_size=(1,10),input_shape=simulation_data.X_train.shape[1::]))\n",
        "multi_filter_keras_model.add(BatchNormalization(axis=-1))\n",
        "multi_filter_keras_model.add(Activation('relu'))\n",
        "multi_filter_keras_model.add(MaxPooling2D(pool_size=(1,35), strides=35))\n",
        "multi_filter_keras_model.add(Flatten())\n",
        "multi_filter_keras_model.add(Dense(1))\n",
        "multi_filter_keras_model.add(Activation(\"sigmoid\"))\n",
        "\n",
        "##compile the model, specifying the Adam optimizer, and binary cross-entropy loss. \n",
        "multi_filter_keras_model.compile(optimizer='adam',\n",
        "                               loss='binary_crossentropy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_JhL10vzhfaQ"
      },
      "source": [
        "multi_filter_keras_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TY_guZFnziif"
      },
      "source": [
        "\"Non-trainable params\" refers to Batch Normalization parameter whose weights don't get updated during training. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rzKZVoNhfaS"
      },
      "source": [
        "##compile the model, specifying the Adam optimizer, and binary cross-entropy loss. \n",
        "multi_filter_keras_model.compile(optimizer='adam',\n",
        "                               loss='binary_crossentropy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xwSZhvVhfaV"
      },
      "source": [
        "We train the model for 150 epochs, with an early stopping criterion -- if the loss on the validation set does not improve for 3 consecutive epochs, the training is halted. In each epoch, the model performs a complete pass over the training data, and updates its parameters to minimize the loss, which quantifies the error in the model predictions. After each epoch, the performance metrics for the model on the validation data were stored. \n",
        "\n",
        "The performance metrics include balanced accuracy, area under the receiver-operating curve ([auROC](https://en.wikipedia.org/wiki/Receiver_operating_characteristic)), are under the precision-recall curve ([auPRC](https://en.wikipedia.org/wiki/Precision_and_recall)), and recall for multiple false discovery rates  (Recall at [FDR](https://en.wikipedia.org/wiki/False_discovery_rate))."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GjRCE1JJhfaV"
      },
      "source": [
        "from dragonn.callbacks import * \n",
        "#We define a custom callback to print training and validation metrics while training. \n",
        "metrics_callback=MetricsCallback(train_data=(simulation_data.X_train,simulation_data.y_train),\n",
        "                                 validation_data=(simulation_data.X_valid,simulation_data.y_valid))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u11Aj6ko2Gyv"
      },
      "source": [
        "We now proceed to train the model. We do this with the keras \"fit\" function. The \"fit\" function has a few key parameters: \n",
        "\n",
        "* **batch_size** -- the number of training and validation samples to be propagated through the network simultaneously. \n",
        "* **epochs** -- An epoch is a measure of the number of times all of the training vectors are used once to update the weights. For batch training all of the training samples pass through the learning algorithm simultaneously in one epoch before weights are updated.\n",
        "* **callbacks** -- Keras callbacks return information from a training algorithm while training is taking place. A callback is a set of functions to be applied at given stages of the training procedure. You can use callbacks to get a view on internal states and statistics of the model during training.\n",
        "* **EarlyStopping** -- a Keras callback that gets called at the end of each epoch. If the loss has not decreased for a consecutive n epochs, where n is referred to as the patience, the training is interrupted. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJT2c7XFhfaX"
      },
      "source": [
        "## use the keras fit function to train the model for 150 epochs with early stopping after 3 epochs \n",
        "history_multi_filter=multi_filter_keras_model.fit(x=simulation_data.X_train,\n",
        "                                  y=simulation_data.y_train,\n",
        "                                  batch_size=128,\n",
        "                                  epochs=150,\n",
        "                                  verbose=1,\n",
        "                                  callbacks=[EarlyStopping(patience=3,restore_best_weights=True),\n",
        "                                            metrics_callback],\n",
        "                                  validation_data=(simulation_data.X_valid,\n",
        "                                                   simulation_data.y_valid))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSmch4Hghfaa"
      },
      "source": [
        "### Evaluate the model on the held-out test set "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCCsY5aohfab"
      },
      "source": [
        "## Use the keras predict function to get model predictions on held-out test set. \n",
        "test_predictions=multi_filter_keras_model.predict(simulation_data.X_test)\n",
        "## Generate a ClassificationResult object to print performance metrics on held-out test set \n",
        "print(ClassificationResult(simulation_data.y_test,test_predictions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSYLNtjjhfaf"
      },
      "source": [
        "### Visualize the model's performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSbvymo7ziiu"
      },
      "source": [
        "#import functions foro visualization of data \n",
        "%matplotlib inline\n",
        "from dragonn.vis import *\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9WSa4OyEhfah"
      },
      "source": [
        "plot_learning_curve(history_multi_filter)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0UFcjPMziiy"
      },
      "source": [
        "We can see that the training and validation loss decrease, but the validation loss is somewhat higher than the training loss. This is indicative of over-fitting to the training data. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UaBJrS2Qhfal"
      },
      "source": [
        "## Visualize the learned parameters "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWMTRBqJhfal"
      },
      "source": [
        "Next, let's visualize the filter learned in this model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u71ItwHBhfan"
      },
      "source": [
        "### Dense layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vwx8sbtuhfan"
      },
      "source": [
        "plot_model_weights(multi_filter_keras_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9Ja44dfhfau"
      },
      "source": [
        "### Convolutional layer "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8v541M5mhfav"
      },
      "source": [
        "W_conv, b_conv = multi_filter_keras_model.layers[0].get_weights()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BuBSKMy-hfax"
      },
      "source": [
        "W_conv.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWVP0eEPhfay"
      },
      "source": [
        "b_conv.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRjf1T--hfa0"
      },
      "source": [
        "plot_filters(multi_filter_keras_model, simulation_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHsr3OughfbB"
      },
      "source": [
        "# Model Interpretation <a name='7'>\n",
        "<a href=#outline>Home</a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8n_EMffXhfbC"
      },
      "source": [
        "As you can see, the filters/model parameters are difficult to be interepreted directly. However, there are alternative approaches of interepreting sequences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzPdGqgDhfbD"
      },
      "source": [
        "Let's examine a positive and negative example from our simulation data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Spz3zpGhfbE"
      },
      "source": [
        "#get the indices of the first positive and negative examples in the validation data split\n",
        "pos_indx=np.flatnonzero(simulation_data.y_valid==1)[0]\n",
        "print(pos_indx)\n",
        "pos_X=simulation_data.X_valid[pos_indx:pos_indx+1]\n",
        "\n",
        "neg_indx=np.flatnonzero(simulation_data.y_valid==0)[0]\n",
        "print(neg_indx)\n",
        "neg_X=simulation_data.X_valid[neg_indx:neg_indx+1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnZ939dqhfbJ"
      },
      "source": [
        "### Motif Scores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ESoHCt9hfbJ"
      },
      "source": [
        "from dragonn.utils import * \n",
        "pos_motif_scores=get_motif_scores(pos_X,simulation_data.motif_names,return_positions=True)\n",
        "neg_motif_scores=get_motif_scores(neg_X,simulation_data.motif_names,return_positions=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAsR6VhphfbL"
      },
      "source": [
        "from dragonn.vis import * \n",
        "plot_motif_scores(pos_motif_scores,title=\"Positive example\",ylim=(0,20))\n",
        "plot_motif_scores(neg_motif_scores,title=\"Negative example\",ylim=(0,20))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYLxiOEShfbP"
      },
      "source": [
        "The motif scan yields a group of three high-scoring motif alignment positions at a fixed distance near the center of the sequence in the positive example. The spacing of the high-scoring motif alignments is random in the negative sequence. \n",
        "\n",
        "Note: If you find that your negative example is too close to the positive examle (i.e. the randomly spaced motifs happen to have a spacing close to the positive example, feel free to provide another index value to select a different negative). \n",
        "\n",
        "For example, you can change the code to select a negative example to the below: \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILWmRtNJzijU"
      },
      "source": [
        "neg_indx=np.flatnonzero(simulation_data.y_valid==0)[10]\n",
        "print(neg_indx)\n",
        "neg_X=simulation_data.X_valid[neg_indx:neg_indx+1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCfzueaWhfbQ"
      },
      "source": [
        "### *In silico* mutagenesis "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAxE_amLhfbQ"
      },
      "source": [
        "To determine how much each position in the input sequence contrinbutes to the model's prediction, we can perform saturation mutagenesis on the sequence. For each position in the input sequence, we introduce each of the four possible bases A, C, G, T and quantify the effect on the model's predictions.\n",
        "\n",
        "*In silico* mutagenesis entails measuring the effect of a given base pair on the model's prediction of accessibility. The following algorithm is used: \n",
        "\n",
        "1. At each position in the input sequence, the reference allele is mutated to each of three possible alternate alleles, and the model predictions with the alternate alleles are obtained. \n",
        "\n",
        "2. The logit values for the reference allele are subtracted from the logit values for each of the 4 alleles. (This means that a difference of 0 will be obtained for the reference allele). We refer to these differences in logit at each position between the reference and alternate alleles as the ISM values. ISM values are computed in logit space to avoid any saturation effects from passing the logits through a sigmoid function. \n",
        "\n",
        "3. For each position, subtract the mean ISM value for that position from each of the 4 ISM values. \n",
        "\n",
        "4. Plot the 4xL heatmap of mean-normalized ISM values \n",
        "\n",
        "5. Plot the reference sequence bases weighted by the highest magnitude ISM score. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ns_TksQkzijZ"
      },
      "source": [
        "pos_X.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-LN7IlUhfbQ"
      },
      "source": [
        "from dragonn.interpret.ism import *\n",
        "ism_pos=in_silico_mutagenesis(multi_filter_keras_model,pos_X,0)\n",
        "ism_neg=in_silico_mutagenesis(multi_filter_keras_model,neg_X,0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6Vya1NBhfbT",
        "scrolled": true
      },
      "source": [
        "%matplotlib inline\n",
        "from dragonn.vis import * \n",
        "\n",
        "# create discrete colormap of ISM scores \n",
        "#zoom into the central 150 bases of the sequence \n",
        "plot_ism(ism_pos,pos_X,title=\"Positive Example\",xlim=(675,825))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOh4lnLOzijg"
      },
      "source": [
        "\n",
        "plot_ism(ism_neg,neg_X,title=\"Negative Example\",xlim=(675,825))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdX_rpL4zijh"
      },
      "source": [
        "We see clear TAL1 motif patterns emerging for the positive example in the central 150 bases of the input sequence; we do not see clear TAL1 patterns in the central 150 bases of the sequence for the negative example. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LElrqg4hhfbX"
      },
      "source": [
        "### Gradient x Input "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sToN6KaqhfbX"
      },
      "source": [
        "Consider a neural net being a function: $f(x_1, ..., x_N; w) = y$\n",
        "\n",
        "One way to tell whether the input feature is important is to compute the gradient of the function with respect to (w.r.t.) model input: $\\frac{\\partial f}{\\partial x_i}$\n",
        "\n",
        "This approach is called saliency maps: \"Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps\", by Karen Simonyan, Andrea Vedaldi and Andrew Zisserma https://arxiv.org/pdf/1312.6034.pdf\n",
        "\n",
        "In genomics, we typically visualize only gradients for bases observed in the sequence (called input masked gradients or input*grad)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Muro8H0Ahfba"
      },
      "source": [
        "from dragonn.interpret.input_grad import * \n",
        "gradinput_pos=input_grad(multi_filter_keras_model,pos_X)\n",
        "gradinput_neg=input_grad(multi_filter_keras_model,neg_X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0p64eg1Ghfbb"
      },
      "source": [
        "from dragonn.vis import plot_seq_importance\n",
        "plot_seq_importance(gradinput_pos,pos_X,title=\"Positive GradXInput\",xlim=(675,825))\n",
        "plot_seq_importance(gradinput_neg,neg_X,title=\"Negative GradXInput\",xlim=(675,825))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1_eQsEshfbg"
      },
      "source": [
        "This confirms what we observed with the ISM analysis -- the positive example contains TAL1 motifs in the central 150 base pairs; the negative example does not. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Votf6uXthfbl"
      },
      "source": [
        "### DeepLIFT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-V9EmMqhfbl"
      },
      "source": [
        "DeepLIFT (cite paper) is another approach to infer the contribution or importance of individual nucleotides in a specific input sequence to its predicted out. While gradients measure the sensitivity of the output to infinitesimal changes in the input, DeepLIFT scores quantify the sensitivity of the output to finite changes in the input. Specifically, the DeepLIFT algorithm backpropagates a score (analogous to gradients) which is based on comparing the activations of all the neurons in the network for the actual input sequence to those obtained when using neutral ‘reference’ sequences. We use dinucleotide-shuffled versions of any input sequence as reference sequences.\n",
        "\n",
        "[DeepLIFT](https://arxiv.org/pdf/1605.01713v2.pdf) allows us to obtain scores for specific sequence indicating the importance of each position in the sequence. DeepLIFT can accept a custom reference. For our purposes, we provide a dinucleotide-shuffled reference. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdfFZvMThfcd"
      },
      "source": [
        "We can now load the saved model for use in other applications or for further fine-tuning:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xoCK-8obhfbn"
      },
      "source": [
        "from dragonn.interpret.deeplift import * \n",
        "#note that the defaults for the deeplift function use 10 shuffled references per input sequence \n",
        "help(deeplift)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PqVkoltzijq"
      },
      "source": [
        "### Saving a keras model \n",
        "\n",
        "\n",
        "We save the optimal regularized multi-layer keras model to an hdf5 file that contains both the model weights and architecture."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_q16v7Qzijr"
      },
      "source": [
        "multi_filter_keras_model.save(\"multi_filter_keras_model.hdf5\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYUPpvChzijt"
      },
      "source": [
        "We can now load the saved model for use in other applications or for further fine-tuning:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6mO6_2Mzijt"
      },
      "source": [
        "from keras.models import load_model\n",
        "model=load_model(\"multi_filter_keras_model.hdf5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9EoqW-Zzijv"
      },
      "source": [
        "We first use the saved model to obtain the DeepLIFT scoring function. We use a shuffled reference, with 10 shuffled reference sequences for each example. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r6GNUM-Jzijv"
      },
      "source": [
        "import dragonn\n",
        "from dragonn.interpret import * \n",
        "help(get_deeplift_scoring_function)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DgOfhw2Tzijz"
      },
      "source": [
        "#target_layer_idx refers to the second-to-last model layer, which is the input to the sigmoid function\n",
        "#task_idx indicates that the first task in a multi-tasked model should be interpreted with DeepLIFT. Because \n",
        "# this is a single-tasked model, the task index will be 0 in all cases. \n",
        "dl_score_func=get_deeplift_scoring_function('multi_filter_keras_model.hdf5',\n",
        "                                           target_layer_idx=-2,\n",
        "                                           task_idx=0,\n",
        "                                           num_refs_per_seq=10,\n",
        "                                           reference='shuffled_ref',\n",
        "                                           one_hot_func=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHTWLXw2zij0"
      },
      "source": [
        "#We use the scoring function to calculate deepLIFT scores for the positive and negative examples \n",
        "dl_pos=deeplift(dl_score_func,pos_X)\n",
        "dl_neg=deeplift(dl_score_func,neg_X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBVn9Gdehfbo"
      },
      "source": [
        "plot_seq_importance(dl_pos,pos_X,title=\"DeepLift positives\",xlim=(675,825))\n",
        "plot_seq_importance(dl_neg,neg_X,title=\"DeepLift negatives\",xlim=(675,825)) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIR0L1BLhfbx"
      },
      "source": [
        "# A multi-layer DragoNN model <a name='8'>\n",
        "\n",
        "<a href=#outline>Home</a> \n",
        "\n",
        "Next, we train a 3 layer model for this task. By adding additional layers, we allow the model to learn more complex features in the sequence data, such as the spatial constraint in the spacing of the TAL1 motifs. \n",
        "\n",
        "However, by adding additional layers to the model, we also make it more likely that the model will overfit to the training data -- we saw that the validation loss was higher than the training loss in the single-layer multi-filter model. We anticipate that this will only be more pronounced with a multi-filter model, so we add some regularization. We regularize the 3 layer using 0.2 dropout on every convolutional layer.\n",
        "![MultiLayerTraining](https://github.com/kundajelab/dragonn/blob/master/tutorials/tutorial_images/MultiLayerTraining.png?raw=1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPFsBPq9hfby"
      },
      "source": [
        "#Define the model architecture in keras\n",
        "\n",
        "regularized_keras_model=Sequential() \n",
        "regularized_keras_model.add(Conv2D(filters=15,kernel_size=(1,10),input_shape=simulation_data.X_train.shape[1::]))\n",
        "regularized_keras_model.add(Activation('relu'))\n",
        "regularized_keras_model.add(Dropout(0.2))\n",
        "\n",
        "regularized_keras_model.add(Conv2D(filters=15,kernel_size=(1,10),input_shape=simulation_data.X_train.shape[1::]))\n",
        "regularized_keras_model.add(Activation('relu'))\n",
        "regularized_keras_model.add(Dropout(0.2))\n",
        "\n",
        "regularized_keras_model.add(Conv2D(filters=15,kernel_size=(1,10),input_shape=simulation_data.X_train.shape[1::]))\n",
        "regularized_keras_model.add(Activation('relu'))\n",
        "regularized_keras_model.add(Dropout(0.2))\n",
        "regularized_keras_model.add(MaxPooling2D(pool_size=(1,35)))\n",
        "\n",
        "\n",
        "regularized_keras_model.add(Flatten())\n",
        "regularized_keras_model.add(Dense(1))\n",
        "regularized_keras_model.add(Activation(\"sigmoid\"))\n",
        "\n",
        "##compile the model, specifying the Adam optimizer, and binary cross-entropy loss. \n",
        "regularized_keras_model.compile(optimizer='adam',\n",
        "                               loss='binary_crossentropy')\n",
        "\n",
        "regularized_keras_model.summary() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5m4Cdeu-zij8"
      },
      "source": [
        "## use the keras fit function to train the model for 150 epochs with early stopping after 3 epochs \n",
        "history_regularized=regularized_keras_model.fit(x=simulation_data.X_train,\n",
        "                                  y=simulation_data.y_train,\n",
        "                                  batch_size=128,\n",
        "                                  epochs=150,\n",
        "                                  verbose=1,\n",
        "                                  callbacks=[EarlyStopping(patience=3,restore_best_weights=True),\n",
        "                                            History(),\n",
        "                                            metrics_callback],\n",
        "                                  validation_data=(simulation_data.X_valid,\n",
        "                                                   simulation_data.y_valid))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGq5nOWCzij-"
      },
      "source": [
        "## Use the keras predict function to get model predictions on held-out test set. \n",
        "test_predictions=regularized_keras_model.predict(simulation_data.X_test)\n",
        "## Generate a ClassificationResult object to print performance metrics on held-out test set \n",
        "print(ClassificationResult(simulation_data.y_test,test_predictions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwPzaEfEzikA"
      },
      "source": [
        "## Visualize the model's performance \n",
        "from dragonn.vis import * \n",
        "plot_learning_curve(history_regularized)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBQBlhxyzikD"
      },
      "source": [
        "regularized_keras_model.save(\"TAL1.Simulation.Regularized.3ConvLayers.hdf5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZohlnVOzikE"
      },
      "source": [
        "Dragonn provides a single utility function to interpret and plot model predictions through all the methodologies we have examined: \n",
        "    * Plots the motif scores (when available) to serve as a \"gold standard\" for interpretation\n",
        "    * Plots the ISM heatmap and sequence importance track \n",
        "    * Plots the gradient x input importance track\n",
        "    * Plots the DeepLIFT importance track \n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCQlVeONzikF"
      },
      "source": [
        "from dragonn.interpret import *\n",
        "help(multi_method_interpret)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTxsACJezikI"
      },
      "source": [
        "#obtain the deepLIFT scoring function for interpretation \n",
        "dl_score_func_multi_layer_regularized=get_deeplift_scoring_function(\"TAL1.Simulation.Regularized.3ConvLayers.hdf5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nw2jPu_RzikJ"
      },
      "source": [
        "from dragonn.vis import * \n",
        "pos_interpretations=multi_method_interpret(regularized_keras_model,\n",
        "                                           pos_X,\n",
        "                                           0,\n",
        "                                           dl_score_func_multi_layer_regularized,\n",
        "                                           motif_names=simulation_data.motif_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrgPkENyzikL"
      },
      "source": [
        "neg_interpretations=multi_method_interpret(regularized_keras_model,\n",
        "                                           neg_X,\n",
        "                                           0,\n",
        "                                           dl_score_func_multi_layer_regularized,\n",
        "                                           motif_names=simulation_data.motif_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7e_CMcFwzikP"
      },
      "source": [
        "We now plot the interpretation scores for pos_X and neg_X along the full sequence as well as along the central 200 bp."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1A_Zs-P8zikP"
      },
      "source": [
        "plot_all_interpretations([pos_interpretations],pos_X,xlim=(675,825))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IkGRCvLPzikS"
      },
      "source": [
        "plot_all_interpretations([neg_interpretations],neg_X,xlim=(675,825))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2vhSeq9hfcc"
      },
      "source": [
        "As expected, additional layers in combination with regularization via dropout lead to improved test set auPRC and decreased overfitting to the training set. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2EK0fK1zikX"
      },
      "source": [
        "## Training on \"Real\" Data: SPI1 TF CHiP-seq data from ENCODE  <a name='9'>\n",
        "<a href=#outline>Home</a>\n",
        "We now want to use DragoNN to train and interpret a neural network on real ENCODE TF-ChIP-seq data. We will learn to predict transcription factor binding for the SPI1 transcription factor in the GM12878 cell line (one of the Tier 1 cell lines for the ENCODE project). \n",
        "    \n",
        "Having done this, we want to examine how well the model is able to predict funcational SNP effects on TF binding. We compare predicted variant effect sizes from classification and regression models against  experimental bQTL data. The bQTL data in this way serves as a \"gold-standard\" validation that in silico mutagenesis on the deep learning inputs leads to correct variant effect size prediction.  We  will use bQTL data  that has been intersected with SPI1 CISBP genome motif annotations. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3g7PZxd7zikX"
      },
      "source": [
        "# SPI1, optimal IDR thresholded peaks, Myers lab, hg19, GM12878 cell type \n",
        "# https://www.encodeproject.org/experiments/ENCSR000BGQ/\n",
        "!wget -O SPI1.narrowPeak.gz https://www.encodeproject.org/files/ENCFF306SRV/@@download/ENCFF306SRV.bed.gz\n",
        "\n",
        "#Fold change bigWig track for the SPI1 dataset: \n",
        "!wget -O SPI1.pooled.fc.bigWig https://www.encodeproject.org/files/ENCFF793RKX/@@download/ENCFF793RKX.bigWig\n",
        "    \n",
        "## Download \"ambiguous\" peak sets -- these peaks are in the optimal overlap set across replicates, but are not\n",
        "## found to be reproducible at a high confidence (p<0.05) by IDR. We have calculated these in advance and download \n",
        "## them from the tutorial server \n",
        "! wget -O SPI1.ambiguous.gz http://mitra.stanford.edu/kundaje/projects/dragonn/SPI1.ambiguous.gz\n",
        "\n",
        "## Download the hg19 chromsizes file (We only use chroms 1 -22, X, Y for training)\n",
        "!wget http://mitra.stanford.edu/kundaje/projects/dragonn/hg19.chrom.sizes\n",
        "    \n",
        "## Download the hg19 fasta reference genome (and corresponding .fai index)\n",
        "!wget http://mitra.stanford.edu/kundaje/projects/dragonn/hg19.genome.fa.gz\n",
        "!wget http://mitra.stanford.edu/kundaje/projects/dragonn/hg19.genome.fa.gz.fai \n",
        "!wget http://mitra.stanford.edu/kundaje/projects/dragonn/hg19.genome.fa.gz.gzi \n",
        "\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEOROObLzikZ"
      },
      "source": [
        "# Download bQTL experimental data for SPI1 loci \n",
        "!wget http://mitra.stanford.edu/kundaje/projects/dragonn/SPI1.bQTLs.txt.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_ML_dROzikb"
      },
      "source": [
        "## Generating genome-wide classification and regression labels <a name='10'>\n",
        "<a href=#outline>Home</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUpOwDibzikc"
      },
      "source": [
        "We used the *genomewide_labels* function from the  [seqdataloader](https://github.com/kundajelab/seqdataloader) package to generate positive and negative labels for the TF-ChIPseq peaks across the genome. For the sake of time, we will load the pre-generated labels in this tutorial, but the label generation code is included below if you would like to run it on your own datasets. \n",
        "\n",
        "seqdataloader splits the genome into 1kb regions, with a stride of 50. Each 1kb region is centered at a 200 bp bin, with a left flank of 400 bases and a right flank of 400 bases. \n",
        "\n",
        "* In the classification case, each 200 bp bin is labeled as positive if a narrowPeak summit overlaps with it. The bin is labeled negative if there is no overlap with the narrowPeak. \n",
        "* In the regression case, the asinh(mean log fold change relative to the input) in the 200 bp bin is computed. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYOuQBLdzikd"
      },
      "source": [
        "##Download pre-generated genomewide labels \n",
        "## Classification labels \n",
        "#! wget http://mitra.stanford.edu/kundaje/projects/dragonn/SPI1.train.classification.hdf5\n",
        "! wget http://mitra.stanford.edu/kundaje/projects/dragonn/SPI1.train.classification.1M.hdf5\n",
        "#! wget http://mitra.stanford.edu/kundaje/projects/dragonn/SPI1.valid.classification.hdf5\n",
        "! wget http://mitra.stanford.edu/kundaje/projects/dragonn/SPI1.valid.classification.1M.hdf5\n",
        "#! wget http://mitra.stanford.edu/kundaje/projects/dragonn/SPI1.test.classification.hdf5\n",
        "! wget http://mitra.stanford.edu/kundaje/projects/dragonn/SPI1.test.classification.1M.hdf5\n",
        "\n",
        "## Regression labels \n",
        "#! wget http://mitra.stanford.edu/kundaje/projects/dragonn/SPI1.train.regression.hdf5\n",
        "! wget http://mitra.stanford.edu/kundaje/projects/dragonn/SPI1.train.regression.1M.hdf5\n",
        "#! wget http://mitra.stanford.edu/kundaje/projects/dragonn/SPI1.valid.regression.hdf5\n",
        "! wget http://mitra.stanford.edu/kundaje/projects/dragonn/SPI1.valid.regression.1M.hdf5\n",
        "#! wget http://mitra.stanford.edu/kundaje/projects/dragonn/SPI1.test.regression.hdf5\n",
        "! wget http://mitra.stanford.edu/kundaje/projects/dragonn/SPI1.test.regression.1M.hdf5\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTMjGoVSzikf"
      },
      "source": [
        "If you are interested in how the labels were generated, you can run the following code: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZVvv1MZzikf"
      },
      "source": [
        "##  Generate genome-wide classification labels \n",
        "#from seqdataloader import * \n",
        "\n",
        "## seqdataloader accepts an input file, which we call SPI1.tasks.tsv, with task names in column 1, corresponding\n",
        "## peak files in column 2, and the signal track in column 3. In this tutorial, the task file will have a single task entry for the SPI1 TF CHiP-seq\n",
        "#with open(\"SPI1.task.tsv\",'w') as f: \n",
        "#    f.write(\"SPI1\\tSPI1.narrowPeak.gz\\tSPI1.pooled.fc.bigWig\\tSPI1.ambiguous.gz\\n\")\n",
        "#f.close() \n",
        "#!cat SPI1.task.tsv\n",
        "\n",
        "##1) Training set: all chromosomes with the exception of 1,2, and 19 in our training set. Also, the dataset does not\n",
        "## include chromosome Y, so we exclude it as well. \n",
        "\n",
        "#train_set_params={\n",
        "#    'task_list':\"SPI1.task.tsv\",\n",
        "#    'outf':\"SPI1.train.classification.hdf5\",\n",
        "#    'output_type':'hdf5',\n",
        "#    'chrom_sizes':'hg19.chrom.sizes',\n",
        "#    'chroms_to_exclude':['chr1','chr2','chr19','chrY'],\n",
        "#    'bin_stride':50,\n",
        "#    'left_flank':400,\n",
        "#    'right_flank':400,\n",
        "#    'bin_size':200,\n",
        "#    'threads':4,\n",
        "#    'subthreads':4,\n",
        "#    'allow_ambiguous':True,\n",
        "#    'labeling_approach':'peak_summit_in_bin_classification'\n",
        "#    }\n",
        "#genomewide_labels(train_set_params)\n",
        "\n",
        "##2) Validation set: Chromosome 1\n",
        "#valid_set_params={'task_list':\"SPI1.task.tsv\",\n",
        "#    'outf':\"SPI1.valid.classification.hdf5\",\n",
        "#    'output_type':'hdf5',\n",
        "#    'chrom_sizes':'hg19.chrom.sizes',\n",
        "#    'chroms_to_keep':'chr1',\n",
        "#    'bin_stride':50,\n",
        "#    'left_flank':400,\n",
        "#    'right_flank':400,\n",
        "#    'bin_size':200,\n",
        "#    'threads':1,\n",
        "#    'subthreads':4,\n",
        "#    'allow_ambiguous':True,\n",
        "#    'labeling_approach':'peak_summit_in_bin_classification'\n",
        "#    }\n",
        "#genomewide_labels(valid_set_params)\n",
        "\n",
        "##3) Test set: Chromosomes 2, 19 \n",
        "#test_set_params={\n",
        "#    'task_list':\"SPI1.task.tsv\",\n",
        "#    'outf':\"SPI1.test.classification.hdf5\",\n",
        "#    'output_type':'hdf5',\n",
        "#    'chrom_sizes':'hg19.chrom.sizes',\n",
        "#    'chroms_to_keep':['chr2','chr19'],\n",
        "#    'bin_stride':50,\n",
        "#    'left_flank':400,\n",
        "#    'right_flank':400,\n",
        "#    'bin_size':200,\n",
        "#    'threads':2,\n",
        "#    'subthreads':4,\n",
        "#    'allow_ambiguous':True,\n",
        "#    'labeling_approach':'peak_summit_in_bin_classification'\n",
        "#    }\n",
        "#genomewide_labels(test_set_params)\n",
        "\n",
        "\n",
        "## Generate regression labels genome-wide \n",
        "\n",
        "##1) Training set: all chromosomes with the exception of 1,2, and 19 in our training set \n",
        "\n",
        "#train_set_params={\n",
        "#    'task_list':\"SPI1.task.tsv\",\n",
        "#    'outf':\"SPI1.train.regression.hdf5\",\n",
        "#    'output_type':'hdf5',\n",
        "#    'chrom_sizes':'hg19.chrom.sizes',\n",
        "#    'chroms_to_exclude':['chr1','chr2','chr19','chrY'],\n",
        "#    'bin_stride':50,\n",
        "#    'left_flank':400,\n",
        "#    'right_flank':400,\n",
        "#    'bin_size':200,\n",
        "#    'threads':4,\n",
        "#    'subthreads':4,\n",
        "#    'allow_ambiguous':True,\n",
        "#    'labeling_approach':'all_genome_bins_regression'\n",
        "#    }\n",
        "#genomewide_labels(train_set_params)\n",
        "\n",
        "##2) Validation set: Chromosome 1\n",
        "#valid_set_params={'task_list':\"SPI1.task.tsv\",\n",
        "#    'outf':\"SPI1.valid.regression.hdf5\",\n",
        "#    'output_type':'hdf5',\n",
        "#    'chrom_sizes':'hg19.chrom.sizes',\n",
        "#    'chroms_to_keep':'chr1',\n",
        "#    'bin_stride':50,\n",
        "#    'left_flank':400,\n",
        "#    'right_flank':400,\n",
        "#    'bin_size':200,\n",
        "#    'threads':1,\n",
        "#    'subthreads':4,\n",
        "#    'allow_ambiguous':True,\n",
        "#    'labeling_approach':'all_genome_bins_regression'\n",
        "#    }\n",
        "#genomewide_labels(valid_set_params)\n",
        "\n",
        "##3) Test set: Chromosomes 2, 19 \n",
        "#test_set_params={\n",
        "#    'task_list':\"SPI1.task.tsv\",\n",
        "#    'outf':\"SPI1.test.regression.hdf5\",\n",
        "#    'output_type':'hdf5',\n",
        "#    'chrom_sizes':'hg19.chrom.sizes',\n",
        "#    'chroms_to_keep':['chr2','chr19'],\n",
        "#    'bin_stride':50,\n",
        "#    'left_flank':400,\n",
        "#    'right_flank':400,\n",
        "#    'bin_size':200,\n",
        "#    'threads':2,\n",
        "#    'subthreads':4,\n",
        "#    'allow_ambiguous':True,\n",
        "#    'labeling_approach':'all_genome_bins_regression'\n",
        "#    }\n",
        "#genomewide_labels(test_set_params)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGtvoGGnziki"
      },
      "source": [
        "Let's examine the label files that were generated: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vdc--xdPziki"
      },
      "source": [
        "#The code generates bed file outputs with a label of 1 or 0 for each 1kb\n",
        "# genome bin for each task. Note that the bins are shifted with a stride of 50.\n",
        "import pandas as pd\n",
        "pd.read_hdf(\"SPI1.train.classification.1M.hdf5\",start=0,end=1000)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFps3yHa2G0x"
      },
      "source": [
        "We will now train genome-wide classification and regression models: \n",
        "![GenomeWideModel](https://github.com/kundajelab/dragonn/blob/master/tutorials/tutorial_images/GenomeWideModel.png?raw=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CabIdVIozikj"
      },
      "source": [
        "## Genome-wide classification model <a name='11'>\n",
        "<a href=#outline>Home</a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5kRuWJMzikk"
      },
      "source": [
        "#To prepare for model training, we import the necessary functions and submodules from keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dropout, Reshape, Dense, Activation, Flatten\n",
        "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
        "from keras.optimizers import Adadelta, SGD, RMSprop;\n",
        "import keras.losses;\n",
        "from keras.constraints import maxnorm;\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.regularizers import l1, l2\n",
        "from keras.callbacks import EarlyStopping, History\n",
        "from keras import backend as K \n",
        "K.set_image_data_format('channels_last')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S33SQU88zikn"
      },
      "source": [
        "from dragonn.runtime_metrics import precision, recall, specificity, fpr, fnr, fdr, f1\n",
        "from keras.constraints import max_norm\n",
        "from dragonn.custom_losses import ambig_binary_crossentropy\n",
        "def initialize_classification_model(ntasks=1):\n",
        "    #Define the model architecture in keras (regularized, 3-layer convolution model followed by 1 dense layer)\n",
        "    model=Sequential() \n",
        "    \n",
        "    model.add(Conv2D(filters=50,kernel_size=(1,15),padding=\"same\", kernel_constraint=max_norm(7.0,axis=-1),input_shape=(1,1000,4)))\n",
        "    model.add(BatchNormalization(axis=-1))\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Conv2D(filters=50,kernel_size=(1,15),padding=\"same\"))\n",
        "    model.add(BatchNormalization(axis=-1))\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Conv2D(filters=50,kernel_size=(1,13),padding=\"same\"))\n",
        "    model.add(BatchNormalization(axis=-1))\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(MaxPooling2D(pool_size=(1,40)))\n",
        "    \n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(50))\n",
        "    model.add(BatchNormalization(axis=-1))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "    \n",
        "    model.add(Dense(ntasks))\n",
        "    model.add(Activation(\"sigmoid\"))\n",
        "    \n",
        "    #use the custom ambig_binary_crossentropy loss, indicating that a value of np.nan indicates an ambiguous label \n",
        "    loss=ambig_binary_crossentropy\n",
        "    \n",
        "    ##compile the model, specifying the Adam optimizer, and binary cross-entropy loss. \n",
        "    model.compile(optimizer='adam', loss=loss,\n",
        "                  metrics=[precision,\n",
        "                          recall,\n",
        "                          specificity,\n",
        "                          fpr,\n",
        "                          fnr,\n",
        "                          fdr,\n",
        "                          f1])\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhv9esVjziko"
      },
      "source": [
        "We create generators for the training and validation data: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kj9kkrUtziko"
      },
      "source": [
        "#create the generators, upsample positives to ensure they constitute 30% of each batch \n",
        "from dragonn.generators import * \n",
        "#spi1_train_classification_gen=DataGenerator(\"SPI1.train.classification.1M.hdf5\",\"hg19.genome.fa.gz\",upsample_ratio=0.3, batch_size=256)\n",
        "#spi1_valid_classification_gen=DataGenerator(\"SPI1.valid.classification.1M.hdf5\",\"hg19.genome.fa.gz\", batch_size=256)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9jZ-5UGzikp"
      },
      "source": [
        "For the sake of time, we will train the model for 10 epochs, using 100 steps per epoch in training and validation.\n",
        "**steps_per_epoch** indicates the number of batches that constitute a single epoch (recall that an epoch constitutes a full forward and backward pass of the dataset through the model before weights are updated). In practice, we often don't need to pass the full dataset through the model to constitute an epoch, especially when the dataset is very large. We can specify the epoch size with the **steps_per_epoch** argument. \n",
        "\n",
        "Use larger values when training an actual model -- a recommended epoch size is 100000 samples for training and 50000 for validation. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_TyA13Kpzikq"
      },
      "source": [
        "#Train the SPI1 classification model \n",
        "#spi1_classification_model=initialize_classification_model()\n",
        "\n",
        "## use the keras fit_generator function to train the model with early stopping after 3 epochs \n",
        "#history_classification=spi1_classification_model.fit_generator(spi1_train_classification_gen,\n",
        "#                                                  validation_data=spi1_valid_classification_gen,\n",
        "#                                                  steps_per_epoch=100, #number of batches to examine in one\n",
        "#                                                  validation_steps=100,\n",
        "#                                                  epochs=10,\n",
        "#                                                  verbose=1,\n",
        "#                                                  use_multiprocessing=True,\n",
        "#                                                  workers=20,\n",
        "#                                                  max_queue_size=100,\n",
        "#                                                  callbacks=[EarlyStopping(patience=3,restore_best_weights=True),History()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncXVdBUfzikr"
      },
      "source": [
        "## Plot the learning curves for SPI1  \n",
        "#plot_learning_curve(history_classification)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGmg8Q7K2G03"
      },
      "source": [
        "The training loss curve decreases gradually over the 7 epochs of training, while the validation loss drops sharply on the first epoch and then remains relatively flat. A possible explanation is that the training set is upsampled to include 10% positives in each batch, while the validation set is not upsampled and therefor has approximately 2% positives in each batch. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QId06rvYziks"
      },
      "source": [
        "#We load the pre-trained SPI1 classification model for further analysis. This model was trained with the same \n",
        "# code as above, but using epochs of size 100000 rather than 100. \n",
        "!wget http://mitra.stanford.edu/kundaje/projects/dragonn/SPI1.classification.model.hdf5\n",
        "\n",
        "from keras.models import load_model\n",
        "from dragonn.custom_losses import * \n",
        "custom_objects={\"recall\":recall,\n",
        "                \"sensitivity\":recall,\n",
        "                \"specificity\":specificity,\n",
        "                \"fpr\":fpr,\n",
        "                \"fnr\":fnr,\n",
        "                \"fdr\":fdr,\n",
        "                \"precision\":precision,\n",
        "                \"f1\":f1,\n",
        "                \"ambig_binary_crossentropy\":ambig_binary_crossentropy}\n",
        "spi1_classification_model=load_model(\"SPI1.classification.model.hdf5\",custom_objects=custom_objects)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kbruUiOzikt"
      },
      "source": [
        "We now measure how well the model performed by calculating performance metrics on the test splits across the whole genome. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5ExyKMNzikt"
      },
      "source": [
        "from dragonn.generators import * \n",
        "spi1_test_classification_gen=DataGenerator(\"SPI1.test.classification.1M.hdf5\",\n",
        "                                         \"hg19.genome.fa.gz\",\n",
        "                                         upsample=False,\n",
        "                                         add_revcomp=False,\n",
        "                                         batch_size=500,\n",
        "                                         tasks=['SPI1'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AboRut1Nziku"
      },
      "source": [
        "spi1_test_classification_predictions=spi1_classification_model.predict_generator(spi1_test_classification_gen,\n",
        "                                                                                 max_queue_size=5000,\n",
        "                                                                                 workers=20,\n",
        "                                                                                 use_multiprocessing=True,\n",
        "                                                                                 verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkNVVN01zikw"
      },
      "source": [
        "spi1_test_classification_labels=spi1_test_classification_gen.data\n",
        "print(spi1_test_classification_predictions.shape) \n",
        "print(spi1_test_classification_labels.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJkhtUlOzikx"
      },
      "source": [
        "#remove nans, as they corresponnd to ambiguous values \n",
        "nan_indices=np.isnan(spi1_test_classification_labels.values.astype(bool))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XI-zkJPiziky"
      },
      "source": [
        "nan_indices.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLTi6xkpzikz"
      },
      "source": [
        "spi1_test_classification_labels=spi1_test_classification_labels[~nan_indices]\n",
        "spi1_test_classification_predictions=np.expand_dims(spi1_test_classification_predictions[~nan_indices],axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBKb08XSzik2"
      },
      "source": [
        "## Generate a ClassificationResult object to print performance metrics on held-out test set \n",
        "from dragonn.metrics import ClassificationResult\n",
        "print(ClassificationResult(spi1_test_classification_labels.values.astype(bool),\n",
        "                           spi1_test_classification_predictions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKNMLQwjzik4"
      },
      "source": [
        "## Genome-wide regression model <a name='12'>\n",
        "<a href=#outline>Home</a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HUkW7Tvzik5"
      },
      "source": [
        "from dragonn.custom_losses import ambig_mean_squared_error\n",
        "def initialize_regression_model(ntasks=1):\n",
        "    #Define the model architecture in keras (regularized, 3-layer convolution model followed by 1 dense layer)\n",
        "    model=Sequential() \n",
        "    \n",
        "    model.add(Conv2D(filters=50,kernel_size=(1,15),padding=\"same\", kernel_constraint=max_norm(7.0,axis=-1),input_shape=(1,1000,4)))\n",
        "    model.add(BatchNormalization(axis=-1))\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Conv2D(filters=50,kernel_size=(1,15),padding=\"same\"))\n",
        "    model.add(BatchNormalization(axis=-1))\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Conv2D(filters=50,kernel_size=(1,13),padding=\"same\"))\n",
        "    model.add(BatchNormalization(axis=-1))\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(MaxPooling2D(pool_size=(1,40)))\n",
        "    \n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(50))\n",
        "    model.add(BatchNormalization(axis=-1))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "    \n",
        "    model.add(Dense(ntasks))\n",
        "\n",
        "    loss=ambig_mean_squared_error\n",
        "    ##compile the model, specifying the Adam optimizer, and binary cross-entropy loss. \n",
        "    model.compile(optimizer='adam',loss=loss)\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YsZcWvp1zik6"
      },
      "source": [
        "We upsample bins with signal greater than 0.1. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xBebnJHzik6"
      },
      "source": [
        "#create the generators, no upsampling of positives is used for regression. \n",
        "#spi1_train_regression_gen=DataGenerator(\"SPI1.train.regression.1M.hdf5\",\"hg19.genome.fa.gz\",upsample_ratio=0.3,upsample_thresh=0.1)\n",
        "#spi1_valid_regression_gen=DataGenerator(\"SPI1.valid.regression.1M.hdf5\",\"hg19.genome.fa.gz\",upsample_thresh=0.1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZretQD22zik7"
      },
      "source": [
        "#Train the SPI1 regression model \n",
        "spi1_regression_model=initialize_regression_model()\n",
        "\n",
        "## use the keras fit_generator function to train the model with early stopping after 3 epochs \n",
        "#history_regression=spi1_regression_model.fit_generator(spi1_train_regression_gen,\n",
        "#                                                  validation_data=spi1_valid_regression_gen,\n",
        "#                                                  steps_per_epoch=100,\n",
        "#                                                  validation_steps=100,\n",
        "#                                                  epochs=10,\n",
        "#                                                  verbose=1,\n",
        "#                                                  use_multiprocessing=True,\n",
        "#                                                  workers=40,\n",
        "#                                                  max_queue_size=100,\n",
        "#                                                  callbacks=[EarlyStopping(patience=3,restore_best_weights=True),History()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NbtfS8Kfzik8"
      },
      "source": [
        "plot_learning_curve(history_regression)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDpmWYfgzik-"
      },
      "source": [
        "#We load a regression model that has been trained on epochs of size 100000 until the early stopping criterion\n",
        "# was met. \n",
        "!wget http://mitra.stanford.edu/kundaje/projects/dragonn/SPI1.regression.model.hdf5\n",
        "\n",
        "from keras.models import load_model\n",
        "from dragonn.custom_losses import * \n",
        "from dragonn.metrics import * \n",
        "custom_objects={\"recall\":recall,\n",
        "                \"sensitivity\":recall,\n",
        "                \"specificity\":specificity,\n",
        "                \"fpr\":fpr,\n",
        "                \"fnr\":fnr,\n",
        "                \"fdr\":fdr,\n",
        "                \"precision\":precision,\n",
        "                \"f1\":f1,\n",
        "                \"ambig_mean_squared_error\":ambig_mean_squared_error}\n",
        "spi1_regression_model=load_model(\"SPI1.regression.model.hdf5\",custom_objects=custom_objects)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UG2lQCIzzik_"
      },
      "source": [
        "#Get predictions on the test set \n",
        "from dragonn.generators import * \n",
        "spi1_test_regression_gen=DataGenerator(\"SPI1.test.regression.1M.hdf5\",\n",
        "                                       \"hg19.genome.fa.gz\",\n",
        "                                         upsample=False,\n",
        "                                         add_revcomp=False,\n",
        "                                         batch_size=1000,\n",
        "                                         tasks=['SPI1'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9BQKMMfzilA"
      },
      "source": [
        "spi1_test_regression_predictions=spi1_regression_model.predict_generator(spi1_test_regression_gen,\n",
        "                                                                         max_queue_size=5000,\n",
        "                                                                         workers=40,\n",
        "                                                                         use_multiprocessing=True,\n",
        "                                                                         verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kyWSQESizilC"
      },
      "source": [
        "spi1_test_regression_labels=spi1_test_regression_gen.data\n",
        "spi1_test_regression_predictions=np.expand_dims(spi1_test_regression_predictions,axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hIF9E7UVzilC"
      },
      "source": [
        "#remove nans, as they correspond to ambiguous values \n",
        "spi1_test_regression_labels=spi1_test_regression_labels[~nan_indices]\n",
        "spi1_test_regression_predictions=spi1_test_regression_predictions[~nan_indices]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88p0zrHKzilD"
      },
      "source": [
        "#Calculate spearman and pearson correlation between truth labels and predictions \n",
        "from scipy.stats import pearsonr, spearmanr\n",
        "corr_pearson=pearsonr(spi1_test_regression_labels,spi1_test_regression_predictions)\n",
        "corr_spearman=spearmanr(spi1_test_regression_labels,spi1_test_regression_predictions)\n",
        "print(\"Pearson correlation on test set:\"+str(corr_pearson))\n",
        "print(\"Spearman correlation on test set:\"+str(corr_spearman))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwHc8XDczilE"
      },
      "source": [
        "## find the indices of the non-zero coverage bins \n",
        "nonzero_bins=spi1_test_regression_labels.max(axis=1)>0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVz7xa5JzilG"
      },
      "source": [
        "# Calculate the spearman and pearson correlation, restricted to non-zero bins \n",
        "corr_pearson_nonzero_bins=pearsonr(spi1_test_regression_labels[nonzero_bins],spi1_test_regression_predictions[nonzero_bins])\n",
        "corr_spearman_nonzero_bins=spearmanr(spi1_test_regression_labels[nonzero_bins],spi1_test_regression_predictions[nonzero_bins])\n",
        "print(\"Pearson correlation on test set:\"+str(corr_pearson_nonzero_bins))\n",
        "print(\"Spearman correlation on test set:\"+str(corr_spearman_nonzero_bins))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9AHLmQ5zilH"
      },
      "source": [
        "## Genome-wide interpretation of true positive predictions in SPI1 <a name='13'>\n",
        "<a href=#outline>Home</a>\n",
        "\n",
        "### Classification Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADKhntlHzilH"
      },
      "source": [
        "#get the true positive predictions with a threshold of 0.9 (i.e. high confidence true positive predictions)\n",
        "true_pos_spi1=spi1_test_classification_labels[spi1_test_classification_labels.values*spi1_test_classification_predictions > 0.9]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yz_cWnuyzilI"
      },
      "source": [
        "true_pos_spi1.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYpN5l5_zilJ"
      },
      "source": [
        "from dragonn.utils import one_hot_from_bed\n",
        "interpretation_input_classification_spi1=one_hot_from_bed([i for i in true_pos_spi1.index],\"hg19.genome.fa.gz\")\n",
        "interpretation_input_classification_spi1.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gooPS9C6zilK"
      },
      "source": [
        "We interpret a high-confidence true  positive example with ISM, gradient analysis, and DeepLIFT. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rr_3bUtFzilL"
      },
      "source": [
        "classification_positive_example= np.expand_dims(interpretation_input_classification_spi1[0],axis=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3pskPAJzilL"
      },
      "source": [
        "from dragonn.interpret import * \n",
        "\n",
        "#we obtain the deepLIFT function to use with this model \n",
        "deeplift_score_func_classification=get_deeplift_scoring_function(\"SPI1.classification.model.hdf5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NEK0gpnezilN"
      },
      "source": [
        "from dragonn.interpret import * \n",
        "\n",
        "pos_interpretations=multi_method_interpret(spi1_classification_model,\n",
        "                                           classification_positive_example,\n",
        "                                           0,\n",
        "                                           deeplift_score_func_classification)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yr3h4jrCzilO"
      },
      "source": [
        "#let's zoom in to the central 200 bp \n",
        "from dragonn.vis import * \n",
        "plot_all_interpretations([pos_interpretations],classification_positive_example,xlim=(400,600))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAewPLWlzilP"
      },
      "source": [
        "If we query the sequence of the observed motif in the [TomTom](http://meme-suite.org/tools/tomtom) software from the MEME suite, we should find  that the motif is a good match for SPIB. \n",
        "An example you might observe is below: \n",
        "<img src=\"https://github.com/kundajelab/dragonn/blob/master/tutorials/tutorial_images/SPI1.Tut4.png?raw=1\" alt=\"SPI12TomTom\" width=\"400\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEwolYHNzilP"
      },
      "source": [
        "### Regression model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KaGQcgJgzilP"
      },
      "source": [
        "#Sanity-check that the model is learning the SPI1 motif by running DeepLIFT on True Positives with high confidence \n",
        "#get the true positive predictions \n",
        "true_pos_regression=spi1_test_regression_labels[(spi1_test_regression_labels.values*spi1_test_regression_predictions)>4]\n",
        "true_pos_regression.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GaRNAEkpzilR"
      },
      "source": [
        "interpretation_input_regression_spi1=one_hot_from_bed([i for i in true_pos_regression.index],\"hg19.genome.fa.gz\")\n",
        "interpretation_input_regression_spi1.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqAZHYvtzilS"
      },
      "source": [
        "regression_positive_example= np.expand_dims(interpretation_input_regression_spi1[0],axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HB3Eqw4pzilS"
      },
      "source": [
        "#get the deeplift scoring function for regression\n",
        "deeplift_score_func_regression=get_deeplift_scoring_function(\"SPI1.regression.model.hdf5\",target_layer_idx=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yMEBuJAzilT"
      },
      "source": [
        "regression_interpretations=multi_method_interpret(spi1_regression_model,\n",
        "                                           regression_positive_example,\n",
        "                                           0,\n",
        "                                           deeplift_score_func_regression)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADwBkACwzilX"
      },
      "source": [
        "plot_all_interpretations([regression_interpretations],regression_positive_example,xlim=(500,600))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8gEXywCzilY"
      },
      "source": [
        "The motif learned by the regression model matches the canonical SPI1 motif as well. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1wFdF75zilY"
      },
      "source": [
        "## SPI1 Binding Quantitative Trait Loci (bQTL) Data <a name='14'>\n",
        "    \n",
        "<a href=#outline>Home</a>\n",
        "\n",
        "A study by Fraser et al (https://www.cell.com/fulltext/S0092-8674(16)30339-7) identified several thousand cis-acting bQTL's that affect the binding of SPI1 transcription factors in humans. We examine how effective our models are at recognizing these bQTLs. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQXpNOMgzilZ"
      },
      "source": [
        "### Read in and annotate the bQTL dataset <a name='a'>\n",
        "<a href=#outline>Home</a>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0465EF4zilZ"
      },
      "source": [
        "## Download the bQTL dataset \n",
        "!wget http://mitra.stanford.edu/kundaje/projects/dragonn/SPI1.bQTLs.txt.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfXlmDRJzild"
      },
      "source": [
        "#Read in the bQTL dataframe\n",
        "bqtls=pd.read_csv(\"SPI1.bQTLs.txt.gz\",header=0,sep='\\t')\n",
        "bqtls.head"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEmIb7UFzile"
      },
      "source": [
        "#Calculate the number of bQTL's\n",
        "print(bqtls.shape)\n",
        "n_bqtls=bqtls.shape[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvR8jl19zilh"
      },
      "source": [
        "#sort the bqtl's by p-value \n",
        "bqtls=bqtls.sort_values(by=['pvalue'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOp7V87Tzili"
      },
      "source": [
        "We now annotate the bQTL's with the false discovery rate, the mean fold change bigWig signal in the bQTL region, and the logratio fo the post-CHIP frequency relative to the pre-CHIP frequency. Some of these operations are time-consuming, particularly the calculation of the mean fold change bigWig signal. Hence,we have pre-run them. If you would like to execute these operations, uncomment the code cells below. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ALWPULyUzili"
      },
      "source": [
        "!wget http://mitra.stanford.edu/kundaje/projects/dragonn/SPI1.bQTLs.additional.metrics.txt.gz\n",
        "bqtls=pd.read_csv(\"SPI1.bQTLs.additional.metrics.txt.gz\",header=0,sep='\\t')\n",
        "bqtls.head"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZZeHn2Qzilj"
      },
      "source": [
        "##Calculate the Benjamini-Hochberg FDR \n",
        "#bqtls['FDR']=1.0\n",
        "#for index,row in bqtls.iterrows(): \n",
        "#    row['FDR']=min(row['FDR'],row['pvalue']*n_bqtls/(index+1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDmzVJWDzilk"
      },
      "source": [
        "## Calculate logratio of the Post-CHIP frequency to the pre-CHIP frequency\n",
        "#bqtls['logratio']=np.log((bqtls['POSTfreq']+.01)/(bqtls['prechipfreq']+0.01))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jA23VgW2zill"
      },
      "source": [
        "## IMPORTANT! Convert the bQTL coordinates from 1-indexing to 0-indexing\n",
        "#bqtls['start']=bqtls['position']-1\n",
        "#bqtls['end']=bqtls['position']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ZumXrJSzill"
      },
      "source": [
        "## Calculate the mean fold change bigWig signal within the 200 bp region \n",
        "## centered on the bQTL\n",
        "\n",
        "##initialize the field within the Pandas dataframe with default value of 0\n",
        "#bqtls['mean_chipseq_fc']=0\n",
        "\n",
        "##use the pyBigWig library to calculate the mean fold change bigWig signal \n",
        "##within the 200 bp region centered at the bQTL \n",
        "#import pyBigWig \n",
        "#bigwig_fh = pyBigWig.open(\"SPI1.pooled.fc.bigWig\")\n",
        "#for index,row in bqtls.iterrows():\n",
        "#    values = bigwig_fh.values(row['Chr'],\n",
        "#                              row['end']-100,\n",
        "#                              row['end']+100,\n",
        "#                              numpy=True)\n",
        "#    row['mean_chipseq_fc'] = np.mean(values)\n",
        "#    if (index%1000 == 0):\n",
        "#        print(\"Done\",index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNHTAvpEziln"
      },
      "source": [
        "##Let's save the augmented bQTL data frame so we can resume from \n",
        "## this point in the analysis more easily. \n",
        "#bqtls.to_csv(\"SPI1.bQTLs.additional.metrics.txt.gz\",\n",
        "#            sep='\\t',\n",
        "#            header=True,\n",
        "#            index=False,\n",
        "#            compression='gzip')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWay4G5azilo"
      },
      "source": [
        "### DNN predictions for Post and Alt alleles within the bQTL dataset  <a name='b'>\n",
        "<a href=#outline>Home</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7eOKuuG2zilo"
      },
      "source": [
        "We would like to obtain  predictions for the reference and alternate alleles at each bQTL. Because we will visualize the difference in predictions for the reference and alternate bQTL alleles, we will operate in logit space -- the layer in the model whose output feeds into the final sigmoid layer. We do this because calculating the logratio of model predictions in logit space is more correct than subtracting probabilities from the model's final sigmoid output layer. \n",
        "\n",
        "Consequently, we first obtain the logits (for the classification model) and pre-activations (for the regression model)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcxGoBUuzilo"
      },
      "source": [
        "# we create a new model that wraps around \"spi1_classification_model\" and provides the logits (layer -2)\n",
        "from keras.models import Model \n",
        "target_layer_idx=-2\n",
        "classification_preact_model=Model(inputs=spi1_classification_model.input,\n",
        "                   outputs=spi1_classification_model.layers[target_layer_idx].output)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HV0M4kezilq"
      },
      "source": [
        "#Create a model to return the preactivation (input to the final ReLU) for the regression model. \n",
        "target_layer_idx=-1\n",
        "regression_preact_model=Model(inputs=spi1_regression_model.input,\n",
        "                   outputs=spi1_regression_model.layers[target_layer_idx].output)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkTEeCV-zilr"
      },
      "source": [
        "# get model predictions for bQTLs \n",
        "from dragonn.generators import * \n",
        "bqtl_post_gen=BQTLGenerator(\"SPI1.bQTLs.txt.gz\",\"hg19.genome.fa.gz\",\"POSTallele\")\n",
        "bqtl_alt_gen=BQTLGenerator(\"SPI1.bQTLs.txt.gz\",\"hg19.genome.fa.gz\",\"ALTallele\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngKLIEmFzilr"
      },
      "source": [
        "bqtl_post_classification_logits=classification_preact_model.predict_generator(bqtl_post_gen,\n",
        "                                                                          max_queue_size=5000,\n",
        "                                                                          workers=40,\n",
        "                                                                          use_multiprocessing=True,\n",
        "                                                                          verbose=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECRwBxb2zils"
      },
      "source": [
        "bqtl_alt_classification_logits=classification_preact_model.predict_generator(bqtl_alt_gen,\n",
        "                                                               max_queue_size=5000, \n",
        "                                                               workers=40, \n",
        "                                                               use_multiprocessing=True, \n",
        "                                                               verbose=1)\n",
        "\n",
        "\n",
        "bqtl_post_classification_truth=bqtl_post_gen.data['pvalue']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x967C1X7zilu"
      },
      "source": [
        "# re-initialize the generators for use in obtaining \n",
        "#predictions from regression models. \n",
        "bqtl_post_gen=BQTLGenerator(\"SPI1.bQTLs.txt.gz\",\"hg19.genome.fa.gz\",\"POSTallele\")\n",
        "bqtl_alt_gen=BQTLGenerator(\"SPI1.bQTLs.txt.gz\",\"hg19.genome.fa.gz\",\"ALTallele\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngU4Zy3ezilv"
      },
      "source": [
        "bqtl_post_regression_preacts=regression_preact_model.predict_generator(bqtl_post_gen,\n",
        "                                                                          max_queue_size=5000,\n",
        "                                                                          workers=40,\n",
        "                                                                          use_multiprocessing=True,\n",
        "                                                                          verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8fxMDKBzily"
      },
      "source": [
        "bqtl_alt_regression_preacts=regression_preact_model.predict_generator(bqtl_alt_gen,\n",
        "                                                                          max_queue_size=5000,\n",
        "                                                                          workers=40,\n",
        "                                                                          use_multiprocessing=True,\n",
        "                                                                          verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRRcztgFzilz"
      },
      "source": [
        "Let's augment the bQTL dataframe with the model predictions. Let's summarize the predictions we have generated on the bQTL dataset: \n",
        "\n",
        "* Classification model logits for the POST allele \n",
        "* Classification model logits for the ALT allele \n",
        "\n",
        "* Regression model pre-activations for the POST allele \n",
        "* Regression model pre-activations for the ALT allele \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVi1GZQJzilz"
      },
      "source": [
        "bqtls['post_classification_logits']=bqtl_post_classification_logits\n",
        "bqtls['alt_classification_logits']=bqtl_alt_classification_logits\n",
        "bqtls['post_regression_preacts']=bqtl_post_regression_preacts\n",
        "bqtls['alt_regression_preacts']=bqtl_alt_regression_preacts\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0NWfRbk_zil0"
      },
      "source": [
        "We want to calibrate out model predictions to follow the same distribution as the inputs. We can do this with Platt scaling for the classifiation model and isotonic regression for the regression models. We do not cover these techniques in this tutorial, but we provide the calibrated predictions in an augmented SPI1 bQTL dataframe: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M58lc_Jzzil0"
      },
      "source": [
        "!wget http://mitra.stanford.edu/kundaje/projects/dragonn/SPI1.bQTLs.augmented.txt.gz\n",
        "bqtls=pd.read_csv(\"SPI1.bQTLs.augmented.txt.gz\",header=0,sep='\\t')\n",
        "bqtls.head"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9XQDDdkzil1"
      },
      "source": [
        "## Compare model predictions on POST and ALT alleles  <a name='c'>\n",
        "<a href=#outline>Home</a>\n",
        "\n",
        "\n",
        "We now assess whether the model predictions of accessibility change for significant bQTL's when the reference allele is replaced with the alternate allele in the input sequence. We also compare the distribution of reference vs alternate predictions for the set of significant bQTLS to the corresponding distribution for the set of non-significant bQTLs. \n",
        "\n",
        "First, we must select comparable subsets of non-significant and significant bQTLs from the bQTL dataset. For our purposes, the sets are comparable if they have similar distributions of accessibility predictions, where predictions are defined as the max of the reference and alternate allele predictions. To ensure that we are comparing such \"matched\" subsets, we perform the following steps:  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vL-VeUSzil1"
      },
      "source": [
        "#We define a temporary field in the bqtl dataframe to store the maximum \n",
        "# of the calibrated classification predictions for the alternate and reference \n",
        "# alleles \n",
        "bqtls['max_classification_predictions']=bqtls[[\"post_classification_logits\",\n",
        "                                               \"alt_classification_logits\"]].max(axis=1)\n",
        "\n",
        "#We define another temporary field to store the maximum of the \n",
        "# calibrated regression predictions for the alternate and reference alleles \n",
        "bqtls['max_regression_predictions']=bqtls[['post_regression_preacts',\n",
        "                                          'alt_regression_preacts']].max(axis=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOJTr6P7zil3"
      },
      "source": [
        "significant_bqtls=bqtls[bqtls['pvalue']<=5e-5]\n",
        "non_significant_bqtls=bqtls[bqtls['pvalue']==1]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Hk5KbgMzil3"
      },
      "source": [
        "significant_bqtls.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIBsz5fzzil5"
      },
      "source": [
        "non_significant_bqtls.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gy3F7rBWzil6"
      },
      "source": [
        "We define a helper function to sample bQTLS from the bQTL dataset to match another subset of bQTLS according to a user-specified attr_name. This function will allow us to compare the accessibility predictions for significant bQTLS with the non-significant bQTLS. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tx8w8VBHzil6"
      },
      "source": [
        "from plotnine import * \n",
        "def plot_distribution(bqtls_to_match,matched_sampled_bqtls,attr_name):\n",
        "    toplot=pd.DataFrame({\"bqtls_to_match\":bqtls_to_match[attr_name],\n",
        "                        \"matched_sampled_bqtls\":matched_sampled_bqtls[attr_name]})\n",
        "    toplot=toplot.melt()\n",
        "    print((ggplot(toplot,aes(x=\"value\",group=\"variable\",color=\"variable\",fill=\"variable\"))\n",
        "     +geom_density(alpha=0.3)\n",
        "     +theme_bw()\n",
        "     +xlab(attr_name)\n",
        "     +ylab(\"Density\")))\n",
        "\n",
        "    print((ggplot(bqtls_to_match,aes(x=attr_name))\n",
        "     +geom_density(alpha=0.3)\n",
        "     +theme_bw()\n",
        "     +xlab(attr_name)\n",
        "     +ylab(\"Density\")\n",
        "     +ggtitle(\"bqtls_to_match\")))\n",
        "\n",
        "    print((ggplot(matched_sampled_bqtls,aes(x=attr_name))\n",
        "     +geom_density(alpha=0.3)\n",
        "     +theme_bw()\n",
        "     +xlab(attr_name)\n",
        "     +ylab(\"Density\")\n",
        "     +ggtitle(\"matched_sampled_bqtls\")))\n",
        "\n",
        "\n",
        "\n",
        "def sample_matched_bqtls(bqtls_to_match, bqtls_to_sample, attr_name):\n",
        "    #sort bqtls_to_sample by attr_name\n",
        "    sorted_bqtls_to_sample=bqtls_to_sample.sort_values(by=[attr_name])\n",
        "    sorted_bqtls_to_sample_vals = sorted_bqtls_to_sample[attr_name]\n",
        "    \n",
        "    bqtls_to_match_vals = bqtls_to_match[attr_name]\n",
        "    \n",
        "    #find indices in the bqtls_to_match_vals Series that are close in value to corresponding entries \n",
        "    # from sorted_bqtls_to_sample_vals\n",
        "    searchsorted_indices = np.searchsorted(a=np.array(sorted_bqtls_to_sample_vals), \n",
        "                                           v=np.array(bqtls_to_match_vals))\n",
        "    \n",
        "    matched_sampled_bqtls_indices = set()\n",
        "    \n",
        "    for idx in searchsorted_indices:\n",
        "        #shift the index until you find one that isn't taken\n",
        "        shift = 1\n",
        "        while (idx in matched_sampled_bqtls_indices or idx==len(sorted_bqtls_to_sample)):\n",
        "            if idx == len(sorted_bqtls_to_sample):\n",
        "                shift = -1\n",
        "            idx += shift\n",
        "        if (idx < 0 or idx > len(sorted_bqtls_to_sample)):\n",
        "            print(idx)\n",
        "        matched_sampled_bqtls_indices.add(idx)\n",
        "    \n",
        "    matched_sampled_bqtls = sorted_bqtls_to_sample.iloc[list(matched_sampled_bqtls_indices)]\n",
        "    \n",
        "    plot_distribution(bqtls_to_match,matched_sampled_bqtls,attr_name)\n",
        "        \n",
        "    return matched_sampled_bqtls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dawg4IPWzil7"
      },
      "source": [
        "We now utilize the \"sample_matched_bqtls\" helper function to select matched subsets of significant and non-significant bQTLs by model prediction values "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LggQaDnczil7"
      },
      "source": [
        "matched_bqtls_maxaltpost = sample_matched_bqtls(bqtls_to_match=significant_bqtls,\n",
        "                                                     bqtls_to_sample=non_significant_bqtls,\n",
        "                                                     attr_name=\"max_classification_predictions\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MnkTPHT3zil8"
      },
      "source": [
        "We have now verified that the distribution of max(alt allele prediction, ref allele predictions) for the significant bQTL subset matches the corresponding distribution for the non-significant bQTL subset. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZdJymUuzil8"
      },
      "source": [
        "Our next step is to generate a scatterplot examining the model predictions for reference vs alternate alleles within the matched significant and non-significant subsets: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIKNKAh0zil8"
      },
      "source": [
        "#concatenate the subset of significant bQTL's and the matched subset of non-significant bQTL's for visualization on the \n",
        "# same plotting axes \n",
        "significant_bqtls['Significant_bQTL']=True\n",
        "matched_bqtls_maxaltpost['Significant_bQTL']=False \n",
        "to_score_bqtls=pd.concat([significant_bqtls,matched_bqtls_maxaltpost],axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCHQZZhyzil9"
      },
      "source": [
        "#concatenate the subset of significant bQTL's and the matched subset of non-significant bQTL's for visualization on the \n",
        "# same plotting axes \n",
        "significant_bqtls['Significant_bQTL']=True\n",
        "matched_bqtls_maxaltpost['Significant_bQTL']=False \n",
        "to_score_bqtls=pd.concat([significant_bqtls,matched_bqtls_maxaltpost],axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zctOjnDfzil_"
      },
      "source": [
        "significant_to_score_bqtls=to_score_bqtls[to_score_bqtls['Significant_bQTL']==True]\n",
        "insignificant_to_score_bqtls=to_score_bqtls[to_score_bqtls['Significant_bQTL']==False]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFxrs2qIzil_"
      },
      "source": [
        "#use the plotnine plotting library to generate a scatterplot of calibrated classification predictions \n",
        "# for the POST and ALT alleles within the significant and matched non-significant bQTL subsets. \n",
        "\n",
        "\n",
        "print((ggplot(significant_to_score_bqtls,aes(x=\"post_classification_predictions\",\n",
        "                y=\"alt_classification_predictions\"))\n",
        "             +geom_point(alpha=0.3,color=\"#0000FF\")\n",
        "             +xlab(\"POST classification predictions\")\n",
        "             +ylab(\"ALT classification predictions\")\n",
        "             +theme_bw()\n",
        "             +ggtitle(\"Significant bQTLS\")))\n",
        "\n",
        "print((ggplot(insignificant_to_score_bqtls,aes(x=\"post_classification_predictions\",\n",
        "                y=\"alt_classification_predictions\"))\n",
        "             +geom_point(alpha=0.3,color=\"#FF0000\")\n",
        "             +xlab(\"POST classification predictions\")\n",
        "             +ylab(\"ALT classification predictions\")\n",
        "             +theme_bw()\n",
        "             +ggtitle(\"Non-Significant bQTLS\")))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H14oQ8wSzimA"
      },
      "source": [
        "As expected, we observe a much higher fraction of significant bQTL's deviating from the diagonal of the plot relative to the non-significant bQTL's. This makes sense -- if a bQTL lies along the diagonal, then the model prediction for the POST allele is very close to the prediction for the ALT allele (i.e. the SNP does not have a strong effect on prediction of accessibility). However, when a bQTL lies far from the diagonal, the model predicts a different probability of chromatin accessibility for the POST and ALT alleles -- i.e. the SNP is disrupting TF binding. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5gUjBXvzimA"
      },
      "source": [
        "## bQTL dataset motif scan with HOCOMOCO SPI1 motif  <a name='d'>\n",
        "<a href=#outline>Home</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yg4kQ--7zimB"
      },
      "source": [
        "Our next step is to annotate the to-score bQTLs with the fasta sequence around them and do motif scoring to determine whether the SPI1 motif is driving the change in model predictions for the reference and altenate alleles."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4HBTlvqzimB"
      },
      "source": [
        "#We save the \"to_score\" bQTL subset to an output file so we can reproduce this analysis in the future starting from the \n",
        "# motif scoring step. \n",
        "to_score_bqtls.to_csv(\"SPI1.bQTLs.toScore.txt.gz\",\n",
        "                      sep='\\t',\n",
        "                      index=False,\n",
        "                      compression=\"gzip\")\n",
        "num_to_score=to_score_bqtls.shape[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWFPP4oYzimC"
      },
      "source": [
        "#We use a flank size of 10 on either side of the bQTL \n",
        "flank_size=10\n",
        "\n",
        "#Once again, we initialize our POST and ALT generators for the bQTL dataset, focusing only on the significant bQTLs and \n",
        "# our matched non-significant subset. \n",
        "#We use a hack where we set the batch size equal to the number of bQTL's to be scored so we get the one-hot-encoding \n",
        "#for all bQTL's simultaneously in one batch \n",
        "bqtl_post_gen=BQTLGenerator(\"SPI1.bQTLs.toScore.txt.gz\",\n",
        "                            \"hg19.genome.fa.gz\",\n",
        "                            \"POSTallele\",\n",
        "                            flank_size=flank_size,\n",
        "                            batch_size=num_to_score)\n",
        "bqtl_alt_gen=BQTLGenerator(\"SPI1.bQTLs.toScore.txt.gz\",\n",
        "                           \"hg19.genome.fa.gz\",\n",
        "                           \"ALTallele\",\n",
        "                           flank_size=flank_size,\n",
        "                           batch_size=num_to_score)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhyXHOudzimE"
      },
      "source": [
        "We now load the SPI1 count matrix from the [HOCOMOCO](http://hocomoco11.autosome.ru/motif/SPI1_HUMAN.H11MO.0.A) database. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSdnXXZFzimF"
      },
      "source": [
        "! [[ -e SPI1.pcm ]] || curl http://hocomoco11.autosome.ru/final_bundle/hocomoco11/full/HUMAN/mono/pcm/SPI1_HUMAN.H11MO.0.A.pcm > SPI1.pcm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLwKiO4AzimG"
      },
      "source": [
        "! cat SPI1.pcm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4UTGZEOzimH"
      },
      "source": [
        "spi1_pcm = np.array([\n",
        "                [float(x) for x in row.rstrip().split(\"\\t\")]\n",
        "                for (i,row) in enumerate(open(\"SPI1.pcm\")) if i > 0]).transpose((1,0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_5Z3JcnzimI"
      },
      "source": [
        "spi1_pcm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "erAS_hxgzimJ"
      },
      "source": [
        "#add a pseudocount for motif normalization \n",
        "pseudocount=10e-4\n",
        "spi1_pcm+=pseudocount\n",
        "spi1_pcm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPCBdAVhzimK"
      },
      "source": [
        "#calculate the frequency matrix from the count matrix \n",
        "#print row-sums \n",
        "np.sum(spi1_pcm,axis=0)\n",
        "#normalize by row-sums \n",
        "spi1_pfm=spi1_pcm/np.sum(spi1_pcm,axis=0)\n",
        "#sanity-check that the row-sums for the position frequency matrix (pfm) are 1 \n",
        "np.sum(spi1_pfm,axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qM-oPb0HzimL"
      },
      "source": [
        "#plot the pwm \n",
        "plot_bases(spi1_pfm.transpose(),figsize=(6,3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GqaxLKQczimM"
      },
      "source": [
        "#plot the reverse complement of the pwm \n",
        "from dragonn.utils import reverse_complement\n",
        "plot_bases(reverse_complement(spi1_pfm.transpose()),figsize=(6,3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f67WWPnazimN"
      },
      "source": [
        "Note that the high confidence true positive SPI1 peaks we interpreted have deepLIFT score profiles resembling the \n",
        "reverse complement SPI1B motif. \n",
        "\n",
        "For each SNP in our bQTL scoring subset, we compute the maximum motif scan score along the 20 bp flanking the reference allele and the alternate allele. In this motif scan, we consider both the motif logo and the reverse complement of the motif logo. We then calculate the difference in the maximum motif score for the alternate allele and the reference allele. These scores should be higher for the significant subset of bQTLs compared to the non-significant subset of bQTLs. We verify that this is indeed the case. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPjzguJ6zimN"
      },
      "source": [
        "In peforming the motif scan, we normalize relative to the gc content of the SPI1 peaks in our dataset. Let's calculate this value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "teEJzCthzimN"
      },
      "source": [
        "# We should have the SPI1.narrowPeak.gz file already downloaded, but just in case  here is the link to download \n",
        "# it again. \n",
        "#!wget http://mitra.stanford.edu/kundaje/projects/dragonn/SPI1.narrowPeak.gz\n",
        "from dragonn.utils import allele_freqs_from_bed\n",
        "spi1_peak_freqs=allele_freqs_from_bed(\"SPI1.narrowPeak.gz\",\"hg19.genome.fa.gz\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQWz-YpjzimO"
      },
      "source": [
        "# a, c, g, t frequqencies \n",
        "print(spi1_peak_freqs)\n",
        "GC_fraction=spi1_peak_freqs[1]+spi1_peak_freqs[2]\n",
        "print(GC_fraction)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGZYyGe9zimP"
      },
      "source": [
        "#Get the maximum sequence SPI1B scan scores for the reference allele \n",
        "to_score_bqtls['scan_post_scores']=np.max(\n",
        "    get_motif_scores(\n",
        "        bqtl_post_gen[0],\n",
        "        [\"SPI1B\"],\n",
        "        GC_fraction=GC_fraction,\n",
        "        pfm=spi1_pfm,\n",
        "        include_rc=True)\n",
        "    ,axis=2).squeeze()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngNt3ExkzimQ"
      },
      "source": [
        "#Get the maximum sequence SPI1B scan scores for the alternate alelle \n",
        "to_score_bqtls['scan_alt_scores']=np.max(\n",
        "    get_motif_scores(\n",
        "        bqtl_alt_gen[0],\n",
        "        [\"SPI1B\"],\n",
        "        GC_fraction=GC_fraction,\n",
        "        pfm=spi1_pfm, \n",
        "        include_rc=True)\n",
        "    ,axis=2).squeeze()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cA1bxJ1NzimQ"
      },
      "source": [
        "#Compute the scan score delta for the reference allele - alternate allele \n",
        "to_score_bqtls['scan_delta']=to_score_bqtls['scan_post_scores']-to_score_bqtls['scan_alt_scores']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGMD3nl9zimR"
      },
      "source": [
        "We summarize our findings below: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJb4iSIvzimR"
      },
      "source": [
        "### bQTL interpretation summary: deep learning models vs motif scan  <a name='e'>\n",
        "<a href=#outline>Home</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_0TkK6-zimR"
      },
      "source": [
        "#### Motif scan "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbVLxEAqzimR"
      },
      "source": [
        "#We plot the motif scan scores for the POST and ALT alleles: \n",
        "significant_to_score_bqtls=to_score_bqtls[to_score_bqtls['Significant_bQTL']==True]\n",
        "insignificant_to_score_bqtls=to_score_bqtls[to_score_bqtls['Significant_bQTL']==False]\n",
        "\n",
        "print((ggplot(significant_to_score_bqtls,aes(x=\"scan_post_scores\",\n",
        "                y=\"scan_alt_scores\"))\n",
        "             +geom_point(alpha=0.3,color=\"#0000FF\")\n",
        "             +xlab(\"POST allele motif scan score for SPI1B\")\n",
        "             +ylab(\"ALT allele motif scan score for SPI1B\")\n",
        "             +ggtitle(\"Significant bQTLs\")\n",
        "             +theme_bw()))\n",
        "print((ggplot(insignificant_to_score_bqtls,aes(x=\"scan_post_scores\",\n",
        "                y=\"scan_alt_scores\"))\n",
        "             +geom_point(alpha=0.3,color=\"#FF0000\")\n",
        "             +xlab(\"POST allele motif scan score for SPI1B\")\n",
        "             +ylab(\"ALT allele motif scan score for SPI1B\")\n",
        "             +ggtitle(\"Non-Significant bQTLs\")\n",
        "             +theme_bw()))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grFeMnarzimS"
      },
      "source": [
        "Recall that we defined the bQTL logratio as the  logratio of the Post-CHIP frequency to the pre-CHIP frequency. Plotting the bQTL logratio vs the delta of the PWM scan score gives: \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ExuBPgY0zimT"
      },
      "source": [
        "print((ggplot(to_score_bqtls,aes(x=\"logratio\",\n",
        "                y=\"scan_delta\",\n",
        "                group=\"Significant_bQTL\",\n",
        "                color=\"Significant_bQTL\"))\n",
        "             +geom_point(alpha=0.3)\n",
        "             +xlab(\"bQTL logratio\")\n",
        "             +ylab(\"POST allele motif scan score - ALT allele motif scan\")\n",
        "             +theme_bw()))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EvY4FTmzimV"
      },
      "source": [
        "#### Calibrated classification model predictions "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQqmuTmTzimW"
      },
      "source": [
        "#We plot the calibrated classification model predictions for the POST and ALT alleles\n",
        "# (we examined this plot above, it is reproduced here for context with the other comparisons)\n",
        "\n",
        "print((ggplot(significant_to_score_bqtls,aes(x=\"post_classification_predictions\",\n",
        "                y=\"alt_classification_predictions\"))\n",
        "             +geom_point(alpha=0.3,color=\"#0000FF\")\n",
        "             +xlab(\"POST allele calibrated classification model prediction\")\n",
        "             +ylab(\"ALT allele calibrated classification model prediction\")\n",
        "             +ggtitle(\"Signicant bQTLs\")\n",
        "             +theme_bw()))\n",
        "print((ggplot(insignificant_to_score_bqtls,aes(x=\"post_classification_predictions\",\n",
        "                y=\"alt_classification_predictions\"))\n",
        "             +geom_point(alpha=0.3,color=\"#FF0000\")\n",
        "             +xlab(\"POST allele calibrated classification model prediction\")\n",
        "             +ylab(\"ALT allele calibrated classification model prediction\")\n",
        "             +ggtitle(\"Non-Signicant bQTLs\")\n",
        "             +theme_bw()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6UMkj8e2zimW"
      },
      "source": [
        "#We plot the logratio for the bQTLs vs the delta in logit space \n",
        "\n",
        "#we extract the fields we need for plotting\n",
        "to_plot=pd.DataFrame({'Significant_bQTL':to_score_bqtls['Significant_bQTL'],\n",
        "                      'logratio':to_score_bqtls['logratio'],\n",
        "                     'delta_classification_predictions':\n",
        "                      to_score_bqtls['post_classification_logits']-\n",
        "                      to_score_bqtls['alt_classification_logits']})\n",
        "\n",
        "print((ggplot(to_plot,aes(x=\"logratio\",\n",
        "                y=\"delta_classification_predictions\",\n",
        "                color=\"Significant_bQTL\",\n",
        "                fill=\"Significant_bQTL\",\n",
        "                group=\"Significant_bQTL\"))\n",
        "             +geom_point(alpha=0.3)\n",
        "             +xlab(\"bQTL logratio\")\n",
        "             +ylab(\"POST allele classification logit - \\nALT allele classification logit\")\n",
        "             +theme_bw()))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpUKBat9zimX"
      },
      "source": [
        "Finally, we restrict our analysis to the subset of bQTL's where the model's prediction changed most dramatically betwen the POST and ALT alleles. For this purpose, we subset the to_score_bqtls to those bQTL's where : \n",
        "\n",
        "* POST calibrated prediction > 0.7 and ALT calibrated prediction < 0.3\n",
        "or \n",
        "* POST calibrated prediction < 0.3 and ALT calibrated prediction > 0.7 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCzn04DgzimX"
      },
      "source": [
        "condition1=(to_score_bqtls['post_classification_predictions']>0.7) & (to_score_bqtls['alt_classification_predictions']<0.3)\n",
        "condition2=(to_score_bqtls['post_classification_predictions']<0.3) & (to_score_bqtls['alt_classification_predictions']>0.7)\n",
        "to_keep=condition1 | condition2 \n",
        "confident_to_score_bqtls=to_score_bqtls[to_keep]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aP98gX1pzimY"
      },
      "source": [
        "confident_to_score_bqtls.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Z0sjF4mzimZ"
      },
      "source": [
        "#We plot the calibrated classification model predictions for the POST and ALT alleles\n",
        "# (we examined this plot above, it is reproduced here for context with the other comparisons)\n",
        "\n",
        "print((ggplot(confident_to_score_bqtls,aes(x=\"post_classification_predictions\",\n",
        "                y=\"alt_classification_predictions\",\n",
        "                group=\"Significant_bQTL\",\n",
        "                color=\"Significant_bQTL\"))\n",
        "             +geom_point(alpha=0.5)\n",
        "             +xlab(\"POST allele calibrated classification model prediction\")\n",
        "             +ylab(\"ALT allele calibrated classification model prediction\")\n",
        "             +scale_color_manual(values=[\"#FF0000\",\"#0000FF\"])\n",
        "             +theme_bw()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXSmTr-ezima"
      },
      "source": [
        "#We plot the logratio for the bQTLs vs the delta in calibrated predictions \n",
        "\n",
        "#we extract the fields we need for plotting\n",
        "to_plot=pd.DataFrame({'Significant_bQTL':confident_to_score_bqtls['Significant_bQTL'],\n",
        "                      'logratio':confident_to_score_bqtls['logratio'],\n",
        "                     'delta_classification_predictions':\n",
        "                      confident_to_score_bqtls['post_classification_logits']-\n",
        "                      confident_to_score_bqtls['alt_classification_logits']})\n",
        "\n",
        "print((ggplot(to_plot,aes(x=\"logratio\",\n",
        "                y=\"delta_classification_predictions\",\n",
        "                color=\"Significant_bQTL\",\n",
        "                fill=\"Significant_bQTL\",\n",
        "                group=\"Significant_bQTL\"))\n",
        "             +geom_point(alpha=0.3)\n",
        "             +xlab(\"bQTL logratio\")\n",
        "             +ylab(\"POST allele classification logit  - \\nALT allele classification logit\")\n",
        "             +scale_color_manual(values=[\"#FF0000\",\"#0000FF\"])\n",
        "             +theme_bw()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCfHTFdLzimc"
      },
      "source": [
        "Our visualizations of the logratio values vs the delta in motif scan scores and model predictions are quite informative. For the significant bQTL's, we see a positive correlation between logratio and delta in model predictions. However, this correlation is nearly non-existent for the motif scan, suggesting the higher sensitivity of the deep learning model. Let's test this out on a low-affinity transcription factor example: \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XR4k9rdIzimc"
      },
      "source": [
        "## Deep learning models are able to identify low-affinity TF-binding sites missed by motif scanning. <a name='15'>\n",
        "<a href=#outline>Home</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9doy8Oq1zimc"
      },
      "source": [
        "Let's examine the region **chr5:107857257:107857288**. We have a highly significant bQTL within that region: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTXzqIvNzimc"
      },
      "source": [
        "to_score_bqtls[(to_score_bqtls['Chr']==\"chr5\") & (to_score_bqtls['position']>107857256) & (to_score_bqtls['position']<107857288)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwlfkTz3zimf"
      },
      "source": [
        "Let's extract the one-hot-encoded 1kb region centered at this position and \n",
        "\n",
        "1) interpret the region with our deep learning models\n",
        "\n",
        "2) scan the region with the canonical SPI1 motif\n",
        "\n",
        "We can compare how the two approaches perform. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iyG938pTzimf"
      },
      "source": [
        "bqtl_post_gen=BQTLGenerator(\"SPI1.bQTLs.toScore.txt.gz\",\n",
        "                            \"hg19.genome.fa.gz\",\n",
        "                            \"POSTallele\",\n",
        "                            batch_size=num_to_score)\n",
        "bqtl_alt_gen=BQTLGenerator(\"SPI1.bQTLs.toScore.txt.gz\",\n",
        "                           \"hg19.genome.fa.gz\",\n",
        "                           \"ALTallele\",\n",
        "                           batch_size=num_to_score)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71kF-udozimg"
      },
      "source": [
        "post_seq=np.expand_dims(bqtl_post_gen[0][1622],axis=0)\n",
        "alt_seq=np.expand_dims(bqtl_alt_gen[0][1622],axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5IQdEscszimg"
      },
      "source": [
        "post_seq_classification_interpretations=multi_method_interpret(spi1_classification_model,\n",
        "                                           post_seq,\n",
        "                                           0,\n",
        "                                           deeplift_score_func_classification,\n",
        "                                           motif_names=['SPI1B'],\n",
        "                                           pfm=spi1_pfm,\n",
        "                                           GC_fraction=GC_fraction)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6T37mmpzimh"
      },
      "source": [
        "plot_all_interpretations([post_seq_classification_interpretations],post_seq,xlim=(450,550),snp_pos=501)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIKD8hmkzimi"
      },
      "source": [
        "DeepLIFT identifies the region centered at the bQTL as important for predicting accessiblity, and the deepLIFT track matches the PWM for SPI1B quite well. The one exception is position 12 in the PWM -- our sequence has a \"T\" at that \n",
        "position, which both the model and the PWM strongly disfavor. However, the model is sensitive enough to recognize that the region is still a weak-affinity SPI1B site, whereas the motif scan lacks that sensitivity -- the motif scan track returns non-significant scores throughout the 1kb region. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1il7AIkLzimi"
      },
      "source": [
        "We repeat the analysis with the regression model: \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJAZq_Wkzimi"
      },
      "source": [
        "post_seq_regression_interpretations=multi_method_interpret(spi1_regression_model,\n",
        "                                           post_seq,\n",
        "                                           0,\n",
        "                                           deeplift_score_func_regression,\n",
        "                                           motif_names=['SPI1B'],\n",
        "                                           pfm=spi1_pfm,\n",
        "                                           GC_fraction=GC_fraction)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--y72wUOzimj"
      },
      "source": [
        "#Zooming in... \n",
        "plot_all_interpretations([post_seq_regression_interpretations],post_seq,xlim=(450,550),snp_pos=501)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Liux1Th6ziml"
      },
      "source": [
        "Although our motif scan gave random-looking scores, let's see how our scan results compare relative to the \"best possible\" motif hit, the \"worst possible\" motif hit, and an \"average\" region selected from the genome. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYUwI8-vziml"
      },
      "source": [
        "#identify indices of the bases with highest probability at each position in the PFM\n",
        "best_match_bases=np.argmax(spi1_pfm,axis=0)\n",
        "worst_match_bases=np.argmin(spi1_pfm,axis=0)\n",
        "mask=np.zeros(spi1_pfm.shape)\n",
        "\n",
        "best_hits=mask.copy() \n",
        "worst_hits=mask.copy() \n",
        "\n",
        "#generate one-hot encoding of the best and worst hits \n",
        "for index in range(mask.shape[1]): \n",
        "    cur_best_base=best_match_bases[index]\n",
        "    cur_worst_base=worst_match_bases[index]\n",
        "    best_hits[cur_best_base,index]=1\n",
        "    worst_hits[cur_worst_base,index]=1\n",
        "\n",
        "#generate a PFM consistent with the genome average base frequencies \n",
        "genome_background=np.tile(np.array([0.26,0.23,0.23,0.26]),(spi1_pfm.shape[1],1)).transpose()\n",
        "\n",
        "#transpose the PFM's and expand dimensions to get a format compatible for motif scanning\n",
        "best_hits=np.expand_dims(np.expand_dims(np.transpose(best_hits),axis=0),axis=0)\n",
        "worst_hits=np.expand_dims(np.expand_dims(np.transpose(worst_hits),axis=0),axis=0)\n",
        "genome_background=np.expand_dims(np.expand_dims(np.transpose(genome_background),axis=0),axis=0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDB7MZmPziml"
      },
      "source": [
        "best_score=np.max(get_motif_scores(best_hits,['SPI1'],GC_fraction=GC_fraction,pfm=spi1_pfm,include_rc=False))\n",
        "print(best_score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxTXgpWgzimm"
      },
      "source": [
        "worst_score=np.min(get_motif_scores(worst_hits,['SPI1'],GC_fraction=GC_fraction,pfm=spi1_pfm,include_rc=False))\n",
        "print(worst_score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6kzJablzimn"
      },
      "source": [
        "genome_background_score=np.max(get_motif_scores(genome_background,['SPI1'],GC_fraction=GC_fraction,pfm=spi1_pfm,include_rc=False))\n",
        "print(genome_background_score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8nnqKlCzimp"
      },
      "source": [
        "How does this compare to our observed score? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Md3OEF3zimp"
      },
      "source": [
        "post_score=get_motif_scores(post_seq,['SPI1'],GC_fraction=GC_fraction,pfm=spi1_pfm,include_rc=True)\n",
        "print(np.max(post_score))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7ICjtInzimq"
      },
      "source": [
        "We overlay the best possible motif match, the worst possible motif match, the observed motif score track, and the genome background score track to illustrate that motif scanning does not effectively identify the SPI1 hit. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqP2-50Hzimq"
      },
      "source": [
        "f,axes=plt.subplots(1,dpi=80,figsize=(20,6))\n",
        "axes.plot(post_score.squeeze(), \"-ko\",label=\"Observed motif scores at low affinity site (1kb region)\")\n",
        "axes.axhline(y=worst_score,color=\"r\",label=\"Worst possible SPI1 score\")\n",
        "axes.axhline(y=best_score,color=\"b\",label=\"Best possible SPI1 score\")\n",
        "axes.axhline(y=genome_background_score,color=\"green\",label=\"Mean genome background  SPI1 score\")\n",
        "axes.axhline(y=np.max(post_score),color=\"orange\",label=\"Max of observed SPI1 scores at low affinity site \")\n",
        "axes.set_xlabel(\"Sequence base\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XyBXLsszimr"
      },
      "source": [
        "In summary, we conclude that the low-affinity SPI1 motif would have been missed by a classical PWM-scanning approach, but is clearly idenfitied via DeepLIFT and ISM analysis on classification and regression models. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7v6zqwHX2G3Z"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}